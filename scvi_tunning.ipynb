{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7717e20d-d8ec-44c1-bc7b-8c6648d5a42b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('raw_training_df.csv')\n",
    "df.set_index('genes', inplace=True)\n",
    "df = df.T\n",
    "\n",
    "metadata = pd.read_csv('metadata.csv')\n",
    "metadata = metadata.set_index('sample_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af8d43c4-5c0c-433d-a518-2c145524cbe5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>genes</th>\n",
       "      <th>ENSG00000000003</th>\n",
       "      <th>ENSG00000000419</th>\n",
       "      <th>ENSG00000000457</th>\n",
       "      <th>ENSG00000000460</th>\n",
       "      <th>ENSG00000000938</th>\n",
       "      <th>ENSG00000000971</th>\n",
       "      <th>ENSG00000001036</th>\n",
       "      <th>ENSG00000001084</th>\n",
       "      <th>ENSG00000001167</th>\n",
       "      <th>ENSG00000001460</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000267784</th>\n",
       "      <th>ENSG00000267786</th>\n",
       "      <th>ENSG00000267787</th>\n",
       "      <th>ENSG00000267791</th>\n",
       "      <th>ENSG00000267795</th>\n",
       "      <th>ENSG00000267796</th>\n",
       "      <th>ENSG00000267797</th>\n",
       "      <th>ENSG00000267799</th>\n",
       "      <th>ENSG00000267800</th>\n",
       "      <th>ENSG00000267801</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1611_GSM2027310</th>\n",
       "      <td>0</td>\n",
       "      <td>290</td>\n",
       "      <td>97</td>\n",
       "      <td>72</td>\n",
       "      <td>11201</td>\n",
       "      <td>12</td>\n",
       "      <td>751</td>\n",
       "      <td>888</td>\n",
       "      <td>771</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611_GSM2027311</th>\n",
       "      <td>5</td>\n",
       "      <td>349</td>\n",
       "      <td>89</td>\n",
       "      <td>71</td>\n",
       "      <td>4231</td>\n",
       "      <td>3</td>\n",
       "      <td>524</td>\n",
       "      <td>910</td>\n",
       "      <td>610</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611_GSM2027312</th>\n",
       "      <td>0</td>\n",
       "      <td>310</td>\n",
       "      <td>611</td>\n",
       "      <td>205</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>249</td>\n",
       "      <td>531</td>\n",
       "      <td>989</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611_GSM2027313</th>\n",
       "      <td>3</td>\n",
       "      <td>337</td>\n",
       "      <td>228</td>\n",
       "      <td>127</td>\n",
       "      <td>2033</td>\n",
       "      <td>4</td>\n",
       "      <td>477</td>\n",
       "      <td>1778</td>\n",
       "      <td>1285</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>183</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611_GSM2027314</th>\n",
       "      <td>0</td>\n",
       "      <td>398</td>\n",
       "      <td>159</td>\n",
       "      <td>106</td>\n",
       "      <td>15270</td>\n",
       "      <td>2</td>\n",
       "      <td>839</td>\n",
       "      <td>1707</td>\n",
       "      <td>952</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>161</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798_Kolf2_iMacs_UT_16h_2_01</th>\n",
       "      <td>38</td>\n",
       "      <td>248</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>241</td>\n",
       "      <td>10</td>\n",
       "      <td>571</td>\n",
       "      <td>211</td>\n",
       "      <td>54</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798_Kolf2_iMacs_UT_16h_3_01</th>\n",
       "      <td>54</td>\n",
       "      <td>271</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>329</td>\n",
       "      <td>5</td>\n",
       "      <td>665</td>\n",
       "      <td>227</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798_Kolf2_iMacs_UT_6h_1_01</th>\n",
       "      <td>37</td>\n",
       "      <td>227</td>\n",
       "      <td>33</td>\n",
       "      <td>38</td>\n",
       "      <td>160</td>\n",
       "      <td>5</td>\n",
       "      <td>722</td>\n",
       "      <td>141</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798_Kolf2_iMacs_UT_6h_2_01</th>\n",
       "      <td>46</td>\n",
       "      <td>261</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>241</td>\n",
       "      <td>8</td>\n",
       "      <td>503</td>\n",
       "      <td>152</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798_Kolf2_iMacs_UT_6h_3_01</th>\n",
       "      <td>63</td>\n",
       "      <td>284</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>437</td>\n",
       "      <td>6</td>\n",
       "      <td>803</td>\n",
       "      <td>337</td>\n",
       "      <td>56</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>554 rows × 27210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "genes                         ENSG00000000003  ENSG00000000419  \\\n",
       "1611_GSM2027310                             0              290   \n",
       "1611_GSM2027311                             5              349   \n",
       "1611_GSM2027312                             0              310   \n",
       "1611_GSM2027313                             3              337   \n",
       "1611_GSM2027314                             0              398   \n",
       "...                                       ...              ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01               38              248   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01               54              271   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01                37              227   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01                46              261   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01                63              284   \n",
       "\n",
       "genes                         ENSG00000000457  ENSG00000000460  \\\n",
       "1611_GSM2027310                            97               72   \n",
       "1611_GSM2027311                            89               71   \n",
       "1611_GSM2027312                           611              205   \n",
       "1611_GSM2027313                           228              127   \n",
       "1611_GSM2027314                           159              106   \n",
       "...                                       ...              ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01               33               28   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01               23               19   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01                33               38   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01                18               36   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01                20               31   \n",
       "\n",
       "genes                         ENSG00000000938  ENSG00000000971  \\\n",
       "1611_GSM2027310                         11201               12   \n",
       "1611_GSM2027311                          4231                3   \n",
       "1611_GSM2027312                            56                1   \n",
       "1611_GSM2027313                          2033                4   \n",
       "1611_GSM2027314                         15270                2   \n",
       "...                                       ...              ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01              241               10   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01              329                5   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01               160                5   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01               241                8   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01               437                6   \n",
       "\n",
       "genes                         ENSG00000001036  ENSG00000001084  \\\n",
       "1611_GSM2027310                           751              888   \n",
       "1611_GSM2027311                           524              910   \n",
       "1611_GSM2027312                           249              531   \n",
       "1611_GSM2027313                           477             1778   \n",
       "1611_GSM2027314                           839             1707   \n",
       "...                                       ...              ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01              571              211   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01              665              227   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01               722              141   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01               503              152   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01               803              337   \n",
       "\n",
       "genes                         ENSG00000001167  ENSG00000001460  ...  \\\n",
       "1611_GSM2027310                           771               25  ...   \n",
       "1611_GSM2027311                           610               23  ...   \n",
       "1611_GSM2027312                           989               78  ...   \n",
       "1611_GSM2027313                          1285               56  ...   \n",
       "1611_GSM2027314                           952               41  ...   \n",
       "...                                       ...              ...  ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01               54                7  ...   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01               50                8  ...   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01                51                5  ...   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01                30                6  ...   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01                56               13  ...   \n",
       "\n",
       "genes                         ENSG00000267784  ENSG00000267786  \\\n",
       "1611_GSM2027310                             0                0   \n",
       "1611_GSM2027311                             0                0   \n",
       "1611_GSM2027312                             0                0   \n",
       "1611_GSM2027313                             0                1   \n",
       "1611_GSM2027314                             0                0   \n",
       "...                                       ...              ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01                0                0   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01                0                0   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01                 0                0   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01                 0                0   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01                 0                0   \n",
       "\n",
       "genes                         ENSG00000267787  ENSG00000267791  \\\n",
       "1611_GSM2027310                           117               23   \n",
       "1611_GSM2027311                            85               16   \n",
       "1611_GSM2027312                           108                9   \n",
       "1611_GSM2027313                           183               22   \n",
       "1611_GSM2027314                           161               26   \n",
       "...                                       ...              ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01                6                0   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01               15                0   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01                12                1   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01                 5                0   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01                 7                0   \n",
       "\n",
       "genes                         ENSG00000267795  ENSG00000267796  \\\n",
       "1611_GSM2027310                             3               86   \n",
       "1611_GSM2027311                             3               88   \n",
       "1611_GSM2027312                             6               74   \n",
       "1611_GSM2027313                             3               74   \n",
       "1611_GSM2027314                             8              110   \n",
       "...                                       ...              ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01                0                1   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01                0                3   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01                 0                3   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01                 0                2   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01                 0                4   \n",
       "\n",
       "genes                         ENSG00000267797  ENSG00000267799  \\\n",
       "1611_GSM2027310                             0                1   \n",
       "1611_GSM2027311                             0                0   \n",
       "1611_GSM2027312                             0                0   \n",
       "1611_GSM2027313                             0                0   \n",
       "1611_GSM2027314                             0                0   \n",
       "...                                       ...              ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01                1                0   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01                1                0   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01                 1                0   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01                 0                0   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01                 0                0   \n",
       "\n",
       "genes                         ENSG00000267800  ENSG00000267801  \n",
       "1611_GSM2027310                             4                2  \n",
       "1611_GSM2027311                             2                4  \n",
       "1611_GSM2027312                             6                6  \n",
       "1611_GSM2027313                             8                2  \n",
       "1611_GSM2027314                            12                2  \n",
       "...                                       ...              ...  \n",
       "9798_Kolf2_iMacs_UT_16h_2_01                0                2  \n",
       "9798_Kolf2_iMacs_UT_16h_3_01                0                1  \n",
       "9798_Kolf2_iMacs_UT_6h_1_01                 0                1  \n",
       "9798_Kolf2_iMacs_UT_6h_2_01                 0                0  \n",
       "9798_Kolf2_iMacs_UT_6h_3_01                 0                1  \n",
       "\n",
       "[554 rows x 27210 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121b577b-c845-4ca5-8d4b-56b1cb956d8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>parental_cell_type</th>\n",
       "      <th>final_cell_type</th>\n",
       "      <th>disease_state</th>\n",
       "      <th>organism</th>\n",
       "      <th>sample_type</th>\n",
       "      <th>tissue_of_origin</th>\n",
       "      <th>sample_name_long</th>\n",
       "      <th>media</th>\n",
       "      <th>...</th>\n",
       "      <th>prep_method_broad</th>\n",
       "      <th>preparation_method</th>\n",
       "      <th>capture</th>\n",
       "      <th>fragmentation</th>\n",
       "      <th>stranded</th>\n",
       "      <th>sequencing_layout</th>\n",
       "      <th>average_read_length</th>\n",
       "      <th>sequencer</th>\n",
       "      <th>training_atlas</th>\n",
       "      <th>sample_id_without_datasetid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1611_GSM2027310</th>\n",
       "      <td>1611</td>\n",
       "      <td>conventional dendritic cell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>cDC2</td>\n",
       "      <td>blood</td>\n",
       "      <td>blood conventional dendritic cell type two (CD...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>illumina_truseq</td>\n",
       "      <td>Illumina Truseq RNA Sample Prep v2</td>\n",
       "      <td>oligo-dT beads</td>\n",
       "      <td>RNA heat and buffer</td>\n",
       "      <td>non stranded</td>\n",
       "      <td>paired</td>\n",
       "      <td>51</td>\n",
       "      <td>Illumina HiSeq 2000</td>\n",
       "      <td>include</td>\n",
       "      <td>GSM2027310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611_GSM2027311</th>\n",
       "      <td>1611</td>\n",
       "      <td>conventional dendritic cell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>cDC1</td>\n",
       "      <td>blood</td>\n",
       "      <td>blood conventional dendritic cell type one (CD...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>illumina_truseq</td>\n",
       "      <td>Illumina Truseq RNA Sample Prep v2</td>\n",
       "      <td>oligo-dT beads</td>\n",
       "      <td>RNA heat and buffer</td>\n",
       "      <td>non stranded</td>\n",
       "      <td>paired</td>\n",
       "      <td>51</td>\n",
       "      <td>Illumina HiSeq 2000</td>\n",
       "      <td>include</td>\n",
       "      <td>GSM2027311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611_GSM2027312</th>\n",
       "      <td>1611</td>\n",
       "      <td>plasmacytoid dendritic cell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>pDC</td>\n",
       "      <td>blood</td>\n",
       "      <td>blood plasmacytoid dendritic cell (CD2-negative)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>illumina_truseq</td>\n",
       "      <td>Illumina Truseq RNA Sample Prep v2</td>\n",
       "      <td>oligo-dT beads</td>\n",
       "      <td>RNA heat and buffer</td>\n",
       "      <td>non stranded</td>\n",
       "      <td>paired</td>\n",
       "      <td>51</td>\n",
       "      <td>Illumina HiSeq 2000</td>\n",
       "      <td>include</td>\n",
       "      <td>GSM2027312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611_GSM2027313</th>\n",
       "      <td>1611</td>\n",
       "      <td>plasmacytoid dendritic cell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>pDC</td>\n",
       "      <td>blood</td>\n",
       "      <td>blood plasmacytoid dendritic cell (CD2-positive)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>illumina_truseq</td>\n",
       "      <td>Illumina Truseq RNA Sample Prep v2</td>\n",
       "      <td>oligo-dT beads</td>\n",
       "      <td>RNA heat and buffer</td>\n",
       "      <td>non stranded</td>\n",
       "      <td>paired</td>\n",
       "      <td>50</td>\n",
       "      <td>Illumina HiSeq 2000</td>\n",
       "      <td>include</td>\n",
       "      <td>GSM2027313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611_GSM2027314</th>\n",
       "      <td>1611</td>\n",
       "      <td>conventional dendritic cell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>cDC2</td>\n",
       "      <td>blood</td>\n",
       "      <td>blood conventional dendritic cell type two (CD...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>illumina_truseq</td>\n",
       "      <td>Illumina Truseq RNA Sample Prep v2</td>\n",
       "      <td>oligo-dT beads</td>\n",
       "      <td>RNA heat and buffer</td>\n",
       "      <td>non stranded</td>\n",
       "      <td>paired</td>\n",
       "      <td>51</td>\n",
       "      <td>Illumina HiSeq 2000</td>\n",
       "      <td>include</td>\n",
       "      <td>GSM2027314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798_Kolf2_iMacs_UT_16h_2_01</th>\n",
       "      <td>9798</td>\n",
       "      <td>macrophage</td>\n",
       "      <td>induced pluripotent stem cell</td>\n",
       "      <td>macrophage</td>\n",
       "      <td>normal</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>UT_16h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>induced pluripotent stem cell derived macropha...</td>\n",
       "      <td>TBC</td>\n",
       "      <td>...</td>\n",
       "      <td>hudson</td>\n",
       "      <td>Hudson</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>include</td>\n",
       "      <td>Kolf2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798_Kolf2_iMacs_UT_16h_3_01</th>\n",
       "      <td>9798</td>\n",
       "      <td>macrophage</td>\n",
       "      <td>induced pluripotent stem cell</td>\n",
       "      <td>macrophage</td>\n",
       "      <td>normal</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>UT_16h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>induced pluripotent stem cell derived macropha...</td>\n",
       "      <td>TBC</td>\n",
       "      <td>...</td>\n",
       "      <td>hudson</td>\n",
       "      <td>Hudson</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>include</td>\n",
       "      <td>Kolf2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798_Kolf2_iMacs_UT_6h_1_01</th>\n",
       "      <td>9798</td>\n",
       "      <td>macrophage</td>\n",
       "      <td>induced pluripotent stem cell</td>\n",
       "      <td>macrophage</td>\n",
       "      <td>normal</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>UT_6h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>induced pluripotent stem cell derived macropha...</td>\n",
       "      <td>TBC</td>\n",
       "      <td>...</td>\n",
       "      <td>hudson</td>\n",
       "      <td>Hudson</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>include</td>\n",
       "      <td>Kolf2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798_Kolf2_iMacs_UT_6h_2_01</th>\n",
       "      <td>9798</td>\n",
       "      <td>macrophage</td>\n",
       "      <td>induced pluripotent stem cell</td>\n",
       "      <td>macrophage</td>\n",
       "      <td>normal</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>UT_6h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>induced pluripotent stem cell derived macropha...</td>\n",
       "      <td>TBC</td>\n",
       "      <td>...</td>\n",
       "      <td>hudson</td>\n",
       "      <td>Hudson</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>include</td>\n",
       "      <td>Kolf2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798_Kolf2_iMacs_UT_6h_3_01</th>\n",
       "      <td>9798</td>\n",
       "      <td>macrophage</td>\n",
       "      <td>induced pluripotent stem cell</td>\n",
       "      <td>macrophage</td>\n",
       "      <td>normal</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>UT_6h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>induced pluripotent stem cell derived macropha...</td>\n",
       "      <td>TBC</td>\n",
       "      <td>...</td>\n",
       "      <td>hudson</td>\n",
       "      <td>Hudson</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>include</td>\n",
       "      <td>Kolf2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>554 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              dataset_id                    cell_type  \\\n",
       "sample_id                                                               \n",
       "1611_GSM2027310                     1611  conventional dendritic cell   \n",
       "1611_GSM2027311                     1611  conventional dendritic cell   \n",
       "1611_GSM2027312                     1611  plasmacytoid dendritic cell   \n",
       "1611_GSM2027313                     1611  plasmacytoid dendritic cell   \n",
       "1611_GSM2027314                     1611  conventional dendritic cell   \n",
       "...                                  ...                          ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01        9798                   macrophage   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01        9798                   macrophage   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01         9798                   macrophage   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01         9798                   macrophage   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01         9798                   macrophage   \n",
       "\n",
       "                                         parental_cell_type final_cell_type  \\\n",
       "sample_id                                                                     \n",
       "1611_GSM2027310                                         NaN             NaN   \n",
       "1611_GSM2027311                                         NaN             NaN   \n",
       "1611_GSM2027312                                         NaN             NaN   \n",
       "1611_GSM2027313                                         NaN             NaN   \n",
       "1611_GSM2027314                                         NaN             NaN   \n",
       "...                                                     ...             ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01  induced pluripotent stem cell      macrophage   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01  induced pluripotent stem cell      macrophage   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01   induced pluripotent stem cell      macrophage   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01   induced pluripotent stem cell      macrophage   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01   induced pluripotent stem cell      macrophage   \n",
       "\n",
       "                             disease_state      organism sample_type  \\\n",
       "sample_id                                                              \n",
       "1611_GSM2027310                     normal  homo sapiens        cDC2   \n",
       "1611_GSM2027311                     normal  homo sapiens        cDC1   \n",
       "1611_GSM2027312                     normal  homo sapiens         pDC   \n",
       "1611_GSM2027313                     normal  homo sapiens         pDC   \n",
       "1611_GSM2027314                     normal  homo sapiens        cDC2   \n",
       "...                                    ...           ...         ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01        normal  homo sapiens      UT_16h   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01        normal  homo sapiens      UT_16h   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01         normal  homo sapiens       UT_6h   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01         normal  homo sapiens       UT_6h   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01         normal  homo sapiens       UT_6h   \n",
       "\n",
       "                             tissue_of_origin  \\\n",
       "sample_id                                       \n",
       "1611_GSM2027310                         blood   \n",
       "1611_GSM2027311                         blood   \n",
       "1611_GSM2027312                         blood   \n",
       "1611_GSM2027313                         blood   \n",
       "1611_GSM2027314                         blood   \n",
       "...                                       ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01              NaN   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01              NaN   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01               NaN   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01               NaN   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01               NaN   \n",
       "\n",
       "                                                               sample_name_long  \\\n",
       "sample_id                                                                         \n",
       "1611_GSM2027310               blood conventional dendritic cell type two (CD...   \n",
       "1611_GSM2027311               blood conventional dendritic cell type one (CD...   \n",
       "1611_GSM2027312                blood plasmacytoid dendritic cell (CD2-negative)   \n",
       "1611_GSM2027313                blood plasmacytoid dendritic cell (CD2-positive)   \n",
       "1611_GSM2027314               blood conventional dendritic cell type two (CD...   \n",
       "...                                                                         ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01  induced pluripotent stem cell derived macropha...   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01  induced pluripotent stem cell derived macropha...   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01   induced pluripotent stem cell derived macropha...   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01   induced pluripotent stem cell derived macropha...   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01   induced pluripotent stem cell derived macropha...   \n",
       "\n",
       "                             media  ... prep_method_broad  \\\n",
       "sample_id                           ...                     \n",
       "1611_GSM2027310                NaN  ...   illumina_truseq   \n",
       "1611_GSM2027311                NaN  ...   illumina_truseq   \n",
       "1611_GSM2027312                NaN  ...   illumina_truseq   \n",
       "1611_GSM2027313                NaN  ...   illumina_truseq   \n",
       "1611_GSM2027314                NaN  ...   illumina_truseq   \n",
       "...                            ...  ...               ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01   TBC  ...            hudson   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01   TBC  ...            hudson   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01    TBC  ...            hudson   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01    TBC  ...            hudson   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01    TBC  ...            hudson   \n",
       "\n",
       "                                              preparation_method  \\\n",
       "sample_id                                                          \n",
       "1611_GSM2027310               Illumina Truseq RNA Sample Prep v2   \n",
       "1611_GSM2027311               Illumina Truseq RNA Sample Prep v2   \n",
       "1611_GSM2027312               Illumina Truseq RNA Sample Prep v2   \n",
       "1611_GSM2027313               Illumina Truseq RNA Sample Prep v2   \n",
       "1611_GSM2027314               Illumina Truseq RNA Sample Prep v2   \n",
       "...                                                          ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01                              Hudson   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01                              Hudson   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01                               Hudson   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01                               Hudson   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01                               Hudson   \n",
       "\n",
       "                                     capture        fragmentation  \\\n",
       "sample_id                                                           \n",
       "1611_GSM2027310               oligo-dT beads  RNA heat and buffer   \n",
       "1611_GSM2027311               oligo-dT beads  RNA heat and buffer   \n",
       "1611_GSM2027312               oligo-dT beads  RNA heat and buffer   \n",
       "1611_GSM2027313               oligo-dT beads  RNA heat and buffer   \n",
       "1611_GSM2027314               oligo-dT beads  RNA heat and buffer   \n",
       "...                                      ...                  ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01         unknown              unknown   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01         unknown              unknown   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01          unknown              unknown   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01          unknown              unknown   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01          unknown              unknown   \n",
       "\n",
       "                                  stranded sequencing_layout  \\\n",
       "sample_id                                                      \n",
       "1611_GSM2027310               non stranded            paired   \n",
       "1611_GSM2027311               non stranded            paired   \n",
       "1611_GSM2027312               non stranded            paired   \n",
       "1611_GSM2027313               non stranded            paired   \n",
       "1611_GSM2027314               non stranded            paired   \n",
       "...                                    ...               ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01       unknown           unknown   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01       unknown           unknown   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01        unknown           unknown   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01        unknown           unknown   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01        unknown           unknown   \n",
       "\n",
       "                             average_read_length            sequencer  \\\n",
       "sample_id                                                               \n",
       "1611_GSM2027310                               51  Illumina HiSeq 2000   \n",
       "1611_GSM2027311                               51  Illumina HiSeq 2000   \n",
       "1611_GSM2027312                               51  Illumina HiSeq 2000   \n",
       "1611_GSM2027313                               50  Illumina HiSeq 2000   \n",
       "1611_GSM2027314                               51  Illumina HiSeq 2000   \n",
       "...                                          ...                  ...   \n",
       "9798_Kolf2_iMacs_UT_16h_2_01             unknown              unknown   \n",
       "9798_Kolf2_iMacs_UT_16h_3_01             unknown              unknown   \n",
       "9798_Kolf2_iMacs_UT_6h_1_01              unknown              unknown   \n",
       "9798_Kolf2_iMacs_UT_6h_2_01              unknown              unknown   \n",
       "9798_Kolf2_iMacs_UT_6h_3_01              unknown              unknown   \n",
       "\n",
       "                             training_atlas sample_id_without_datasetid  \n",
       "sample_id                                                                \n",
       "1611_GSM2027310                     include                  GSM2027310  \n",
       "1611_GSM2027311                     include                  GSM2027311  \n",
       "1611_GSM2027312                     include                  GSM2027312  \n",
       "1611_GSM2027313                     include                  GSM2027313  \n",
       "1611_GSM2027314                     include                  GSM2027314  \n",
       "...                                     ...                         ...  \n",
       "9798_Kolf2_iMacs_UT_16h_2_01        include                       Kolf2  \n",
       "9798_Kolf2_iMacs_UT_16h_3_01        include                       Kolf2  \n",
       "9798_Kolf2_iMacs_UT_6h_1_01         include                       Kolf2  \n",
       "9798_Kolf2_iMacs_UT_6h_2_01         include                       Kolf2  \n",
       "9798_Kolf2_iMacs_UT_6h_3_01         include                       Kolf2  \n",
       "\n",
       "[554 rows x 35 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5117c9a-2be5-4412-8942-30f1caa84c2d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bio_tag = 'cell_type'\n",
    "batch_tag = 'prep_method_broad'\n",
    "label_tag = 'cell_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba436073-d2cd-4731-9b63-ca602e1938bd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "adata = sc.AnnData(df)\n",
    "adata.obs = adata.obs.merge(metadata, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "284ca061-5816-45ac-b63c-df6f6fa9086c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 554 × 27210\n",
       "    obs: 'dataset_id', 'cell_type', 'parental_cell_type', 'final_cell_type', 'disease_state', 'organism', 'sample_type', 'tissue_of_origin', 'sample_name_long', 'media', 'cell_line', 'registry', 'facs_profile_positive', 'facs_profile_negative', 'sample_description', 'experiment_time', 'sex', 'reprogramming_method', 'genetic_modification', 'sample_source', 'developmental_stage', 'treatment', 'external_source_id', 'platform_type', 'pre_amplification', 'prep_method_broad', 'preparation_method', 'capture', 'fragmentation', 'stranded', 'sequencing_layout', 'average_read_length', 'sequencer', 'training_atlas', 'sample_id_without_datasetid'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "290f8b26-9676-4897-b2f8-9d0252897ea1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import scvi\n",
    "from scvi import autotune\n",
    "\n",
    "model = scvi.model.SCVI\n",
    "model.setup_anndata(adata, batch_key = batch_tag, labels_key = label_tag)\n",
    "\n",
    "scvi_tuner = autotune.ModelTuner(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a23bd921-6e22-464d-87aa-5311f8950627",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ModelTuner registry for SCVI\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ModelTuner registry for SCVI\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                  Tunable hyperparameters                  </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Hyperparameter      </span>┃<span style=\"font-weight: bold\"> Default value </span>┃<span style=\"font-weight: bold\">    Source    </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">         n_hidden         </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">      128      </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     VAE      </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">         n_latent         </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">      10       </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     VAE      </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">         n_layers         </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">       1       </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     VAE      </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">       dropout_rate       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">      0.1      </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     VAE      </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">        dispersion        </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     gene      </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     VAE      </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">     log_variational      </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     True      </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     VAE      </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">     gene_likelihood      </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     zinb      </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     VAE      </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">   latent_distribution    </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">    normal     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     VAE      </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">    encode_covariates     </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     False     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     VAE      </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\"> deeply_inject_covariates </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     True      </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     VAE      </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">      use_batch_norm      </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     both      </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     VAE      </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">      use_layer_norm      </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     none      </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     VAE      </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">  use_observed_lib_size   </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     True      </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     VAE      </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">      var_activation      </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     None      </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     VAE      </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">        optimizer         </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     Adam      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> TrainingPlan </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">            lr            </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     0.001     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> TrainingPlan </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">       weight_decay       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     1e-06     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> TrainingPlan </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">           eps            </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     0.01      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> TrainingPlan </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">    n_steps_kl_warmup     </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     None      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> TrainingPlan </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">    n_epochs_kl_warmup    </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">      400      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> TrainingPlan </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">   reduce_lr_on_plateau   </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     False     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> TrainingPlan </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">        lr_factor         </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">      0.6      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> TrainingPlan </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">       lr_patience        </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">      30       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> TrainingPlan </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">       lr_threshold       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">      0.0      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> TrainingPlan </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">          lr_min          </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">       0       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> TrainingPlan </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">      max_kl_weight       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">      1.0      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> TrainingPlan </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">      min_kl_weight       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">      0.0      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> TrainingPlan </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">        batch_size        </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">      128      </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     SCVI     </span>│\n",
       "└──────────────────────────┴───────────────┴──────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                  Tunable hyperparameters                  \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Hyperparameter     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDefault value\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Source   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m        n_hidden        \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m     128     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    VAE     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m        n_latent        \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m     10      \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    VAE     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m        n_layers        \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m      1      \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    VAE     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m      dropout_rate      \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m     0.1     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    VAE     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m       dispersion       \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    gene     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    VAE     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m    log_variational     \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    True     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    VAE     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m    gene_likelihood     \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    zinb     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    VAE     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m  latent_distribution   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m   normal    \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    VAE     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m   encode_covariates    \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    False    \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    VAE     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33mdeeply_inject_covariates\u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    True     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    VAE     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m     use_batch_norm     \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    both     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    VAE     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m     use_layer_norm     \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    none     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    VAE     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m use_observed_lib_size  \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    True     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    VAE     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m     var_activation     \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    None     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    VAE     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m       optimizer        \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    Adam     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrainingPlan\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m           lr           \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    0.001    \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrainingPlan\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m      weight_decay      \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    1e-06    \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrainingPlan\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m          eps           \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    0.01     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrainingPlan\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m   n_steps_kl_warmup    \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    None     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrainingPlan\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m   n_epochs_kl_warmup   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m     400     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrainingPlan\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m  reduce_lr_on_plateau  \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    False    \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrainingPlan\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m       lr_factor        \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m     0.6     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrainingPlan\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m      lr_patience       \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m     30      \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrainingPlan\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m      lr_threshold      \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m     0.0     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrainingPlan\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m         lr_min         \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m      0      \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrainingPlan\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m     max_kl_weight      \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m     1.0     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrainingPlan\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m     min_kl_weight      \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m     0.0     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mTrainingPlan\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m       batch_size       \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m     128     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    SCVI    \u001b[0m\u001b[32m \u001b[0m│\n",
       "└──────────────────────────┴───────────────┴──────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">       Available metrics        </span>\n",
       "┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">     Metric      </span>┃<span style=\"font-weight: bold\">    Mode    </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\"> validation_loss </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">    min     </span>│\n",
       "└─────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m       Available metrics        \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m    Metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Mode   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33mvalidation_loss\u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m   min    \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "└─────────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                         Default search space                         </span>\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Hyperparameter </span>┃<span style=\"font-weight: bold\"> Sample function </span>┃<span style=\"font-weight: bold\">  Arguments  </span>┃<span style=\"font-weight: bold\"> Keyword arguments </span>┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">    n_hidden    </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">     choice      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> [[64, 128]] </span>│<span style=\"color: #ff8700; text-decoration-color: #ff8700\">        {}         </span>│\n",
       "└────────────────┴─────────────────┴─────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                         Default search space                         \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mHyperparameter\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSample function\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Arguments \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mKeyword arguments\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m   n_hidden   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m    choice     \u001b[0m\u001b[38;5;128m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m[[64, 128]]\u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;208m \u001b[0m\u001b[38;5;208m       {}        \u001b[0m\u001b[38;5;208m \u001b[0m│\n",
       "└────────────────┴─────────────────┴─────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scvi_tuner.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2913b1ee-01f3-4206-b5c5-5b3533667149",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-08-06 16:16:53</td></tr>\n",
       "<tr><td>Running for: </td><td>01:36:05.69        </td></tr>\n",
       "<tr><td>Memory:      </td><td>101.9/1006.9 GiB   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=551<br>Bracket: Iter 64.000: -135361.3046875 | Iter 32.000: -150319.21875 | Iter 16.000: -172104.796875 | Iter 8.000: -200089.5390625 | Iter 4.000: -223021.890625 | Iter 2.000: -237859.5625 | Iter 1.000: -247743.921875<br>Logical resource usage: 64.0/64 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:H100)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  ... 538 more trials not shown (538 TERMINATED)\n",
       "  Number of errored trials: 6<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                                                                                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>_trainable_42914ad4</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-08-06_14-40-35_507651_124852/artifacts/2024-08-06_14-40-47/2024-08-06_14-40-35_scvi/driver_artifacts/_trainable_42914ad4_5_dispersion=gene-label,dropout_rate=0.2000,gene_likelihood=nb,lr=0.0089,n_hidden=2048,n_latent=3,n_layers=7_2024-08-06_14-41-32/error.txt  </td></tr>\n",
       "<tr><td>_trainable_3be28bbe</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-08-06_14-40-35_507651_124852/artifacts/2024-08-06_14-40-47/2024-08-06_14-40-35_scvi/driver_artifacts/_trainable_3be28bbe_230_dispersion=gene-label,dropout_rate=0.1000,gene_likelihood=nb,lr=0.0070,n_hidden=2048,n_latent=3,n_layers=4_2024-08-06_15-21-26/error.txt</td></tr>\n",
       "<tr><td>_trainable_3e2401fb</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-08-06_14-40-35_507651_124852/artifacts/2024-08-06_14-40-47/2024-08-06_14-40-35_scvi/driver_artifacts/_trainable_3e2401fb_232_dispersion=gene-label,dropout_rate=0.1000,gene_likelihood=nb,lr=0.0073,n_hidden=2048,n_latent=3,n_layers=4_2024-08-06_15-21-46/error.txt</td></tr>\n",
       "<tr><td>_trainable_0d6ac7a3</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-08-06_14-40-35_507651_124852/artifacts/2024-08-06_14-40-47/2024-08-06_14-40-35_scvi/driver_artifacts/_trainable_0d6ac7a3_331_dispersion=gene-label,dropout_rate=0.2000,gene_likelihood=nb,lr=0.0076,n_hidden=2048,n_latent=3,n_layers=1_2024-08-06_15-38-11/error.txt</td></tr>\n",
       "<tr><td>_trainable_bd5ede2c</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-08-06_14-40-35_507651_124852/artifacts/2024-08-06_14-40-47/2024-08-06_14-40-35_scvi/driver_artifacts/_trainable_bd5ede2c_505_dispersion=gene-label,dropout_rate=0.2500,gene_likelihood=nb,lr=0.0091,n_hidden=2048,n_latent=3,n_layers=9_2024-08-06_16-07-49/error.txt</td></tr>\n",
       "<tr><td>_trainable_5b397612</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-08-06_14-40-35_507651_124852/artifacts/2024-08-06_14-40-47/2024-08-06_14-40-35_scvi/driver_artifacts/_trainable_5b397612_507_dispersion=gene-label,dropout_rate=0.2500,gene_likelihood=nb,lr=0.0092,n_hidden=2048,n_latent=3,n_layers=9_2024-08-06_16-08-09/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  n_hidden</th><th style=\"text-align: right;\">  n_layers</th><th style=\"text-align: right;\">  n_latent</th><th style=\"text-align: right;\">  dropout_rate</th><th>dispersion  </th><th>gene_likelihood  </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  validation_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>_trainable_00c20f7c</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">         5</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.3 </td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00878003 </td><td style=\"text-align: right;\">                 </td></tr>\n",
       "<tr><td>_trainable_00375873</td><td>TERMINATED</td><td>172.26.93.179:124818</td><td style=\"text-align: right;\">      2048</td><td style=\"text-align: right;\">         9</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.15</td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00520279 </td><td style=\"text-align: right;\">      2.86619e+10</td></tr>\n",
       "<tr><td>_trainable_003a3cdd</td><td>TERMINATED</td><td>172.26.93.179:224140</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.2 </td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00182017 </td><td style=\"text-align: right;\"> 252410          </td></tr>\n",
       "<tr><td>_trainable_014193bf</td><td>TERMINATED</td><td>172.26.93.179:38995 </td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.2 </td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00406868 </td><td style=\"text-align: right;\"> 182036          </td></tr>\n",
       "<tr><td>_trainable_01dd7e2a</td><td>TERMINATED</td><td>172.26.93.179:92464 </td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.2 </td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00336477 </td><td style=\"text-align: right;\"> 241712          </td></tr>\n",
       "<tr><td>_trainable_01f6d229</td><td>TERMINATED</td><td>172.26.93.179:149233</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.25</td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00411212 </td><td style=\"text-align: right;\"> 240906          </td></tr>\n",
       "<tr><td>_trainable_0260545f</td><td>TERMINATED</td><td>172.26.93.179:98504 </td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">         7</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.25</td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00996901 </td><td style=\"text-align: right;\"> 127431          </td></tr>\n",
       "<tr><td>_trainable_031afc3f</td><td>TERMINATED</td><td>172.26.93.179:97672 </td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">         7</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.25</td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00880444 </td><td style=\"text-align: right;\"> 131728          </td></tr>\n",
       "<tr><td>_trainable_0346e3e4</td><td>TERMINATED</td><td>172.26.93.179:150452</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.25</td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00634653 </td><td style=\"text-align: right;\"> 136491          </td></tr>\n",
       "<tr><td>_trainable_035ded50</td><td>TERMINATED</td><td>172.26.93.179:74698 </td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.15</td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00204341 </td><td style=\"text-align: right;\"> 245263          </td></tr>\n",
       "<tr><td>_trainable_0385c7ea</td><td>TERMINATED</td><td>172.26.93.179:147898</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.25</td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00730668 </td><td style=\"text-align: right;\"> 125286          </td></tr>\n",
       "<tr><td>_trainable_038c4837</td><td>TERMINATED</td><td>172.26.93.179:19923 </td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">         8</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.1 </td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00712243 </td><td style=\"text-align: right;\">      5.03514e+08</td></tr>\n",
       "<tr><td>_trainable_0398a046</td><td>TERMINATED</td><td>172.26.93.179:191878</td><td style=\"text-align: right;\">        64</td><td style=\"text-align: right;\">         8</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.2 </td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00376748 </td><td style=\"text-align: right;\"> 239092          </td></tr>\n",
       "<tr><td>_trainable_04305e95</td><td>TERMINATED</td><td>172.26.93.179:139683</td><td style=\"text-align: right;\">       128</td><td style=\"text-align: right;\">         6</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.25</td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.000320343</td><td style=\"text-align: right;\"> 215451          </td></tr>\n",
       "<tr><td>_trainable_0d6ac7a3</td><td>ERROR     </td><td>172.26.93.179:90119 </td><td style=\"text-align: right;\">      2048</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.2 </td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00757306 </td><td style=\"text-align: right;\">    nan          </td></tr>\n",
       "<tr><td>_trainable_3be28bbe</td><td>ERROR     </td><td>172.26.93.179:13155 </td><td style=\"text-align: right;\">      2048</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.1 </td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00695521 </td><td style=\"text-align: right;\">    nan          </td></tr>\n",
       "<tr><td>_trainable_3e2401fb</td><td>ERROR     </td><td>172.26.93.179:14503 </td><td style=\"text-align: right;\">      2048</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.1 </td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00727808 </td><td style=\"text-align: right;\">    nan          </td></tr>\n",
       "<tr><td>_trainable_42914ad4</td><td>ERROR     </td><td>172.26.93.179:137323</td><td style=\"text-align: right;\">      2048</td><td style=\"text-align: right;\">         7</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.2 </td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00894805 </td><td style=\"text-align: right;\">                 </td></tr>\n",
       "<tr><td>_trainable_5b397612</td><td>ERROR     </td><td>172.26.93.179:227159</td><td style=\"text-align: right;\">      2048</td><td style=\"text-align: right;\">         9</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.25</td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00917355 </td><td style=\"text-align: right;\">    nan          </td></tr>\n",
       "<tr><td>_trainable_bd5ede2c</td><td>ERROR     </td><td>172.26.93.179:225920</td><td style=\"text-align: right;\">      2048</td><td style=\"text-align: right;\">         9</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">          0.25</td><td>gene-label  </td><td>nb               </td><td style=\"text-align: right;\">0.00905865 </td><td style=\"text-align: right;\">                 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.9/subprocess.py:1770: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "/usr/lib64/python3.9/subprocess.py:1770: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "2024-08-06 14:40:45,210\tINFO worker.py:1740 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2024-08-06 14:40:47,345\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
      "/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/tune.py:583: UserWarning: The `local_dir` argument is deprecated and will be removed. This will pass-through to set the `storage_path` for now but will raise an error in the future. You should only set the `storage_path` from now on.\n",
      "  warnings.warn(\n",
      "2024-08-06 14:40:47,385\tINFO tune.py:614 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\u001b[36m(_trainable pid=132795)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=132795)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=132795)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=132795)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=132795)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=132795)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=132795)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=132795)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=132795)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1c513b2a_lightning\n",
      "\u001b[36m(_trainable pid=132795)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=132795)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=132795)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=132795)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4e1d4690_lightning\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m [2024-08-06 14:41:16,686 E 134292 134878] logging.cc:108: Stack trace: \n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m  /home/chzhan1/.local/lib/python3.9/site-packages/ray/_raylet.so(+0x1021b3a) [0x14932f5f4b3a] ray::operator<<()\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/_raylet.so(+0x10245f8) [0x14932f5f75f8] ray::TerminateHandler()\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /apps/easybuild-2022/easybuild/software/Core/GCCcore/11.3.0/lib64/libstdc++.so.6(+0xb4f9a) [0x14932e466f9a] __cxxabiv1::__terminate()\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /apps/easybuild-2022/easybuild/software/Core/GCCcore/11.3.0/lib64/libstdc++.so.6(+0xb5005) [0x14932e467005]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /apps/easybuild-2022/easybuild/software/Core/GCCcore/11.3.0/lib64/libstdc++.so.6(__gxx_personality_v0+0x2bc) [0x14932e46695c] __gxx_personality_v0\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /apps/easybuild-2022/easybuild/software/Core/GCCcore/11.3.0/lib64/libgcc_s.so.1(+0x115c4) [0x14932e3a95c4] _Unwind_ForcedUnwind_Phase2\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /apps/easybuild-2022/easybuild/software/Core/GCCcore/11.3.0/lib64/libgcc_s.so.1(_Unwind_ForcedUnwind+0x132) [0x14932e3a9cb2] _Unwind_ForcedUnwind\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libc.so.6(+0x92406) [0x149330092406] __GI___pthread_unwind\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libc.so.6(pthread_exit+0x3a) [0x14933008adba] pthread_exit\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0xc9845) [0x1493304c9845]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x614e1) [0x1493304614e1]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(PyEval_RestoreThread+0x16) [0x14933052e166] PyEval_RestoreThread\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so(+0x570745) [0x1463a8b66745] torch::autograd::dispatch_to()\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so(+0x570a0e) [0x1463a8b66a0e] torch::autograd::THPVariable_cpu()\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x11ce51) [0x14933051ce51]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x757) [0x14933050fe37] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x14933050e725]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x14933051bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x47a) [0x14933050fb5a] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x14933050e725]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x14933051bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1493305246e1]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1493305147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x14933050e725]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x14933051bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1493305246e1]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1493305147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x14933050e725]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x14933051bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1493305246e1]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1493305147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x14933050e725]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x14933051bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1493305246e1]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1493305147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x14933050e725]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x14933051bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1493305246e1]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1493305147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x14933050e725]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x14933051bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1493305246e1]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1493305147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x14933050e725]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x14933051bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1493305246e1]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1493305147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x11c223) [0x14933051c223]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1493305246e1]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1493305147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x14933050e725]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x14933051bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1493305246e1]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1493305147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x11c223) [0x14933051c223]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x757) [0x14933050fe37] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x11c223) [0x14933051c223]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x757) [0x14933050fe37] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x14933050e725]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x14933051bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1493305147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x14933050e725]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x14933051bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m /lib64/libpython3.9.so.1.0(+0x124782) [0x149330524782]\n",
      "\u001b[36m(_trainable pid=134292)\u001b[0m \n",
      "\u001b[36m(_trainable pid=135429)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=135429)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=135429)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=135429)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=135429)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=135429)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=135429)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=135429)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=135429)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_217bca6a_lightning\n",
      "\u001b[36m(_trainable pid=135429)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=135429)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=135429)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=135429)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=136282)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=136282)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=136282)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=136282)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=136282)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=136282)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=136282)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=136282)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=136282)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1b242665_lightning\n",
      "\u001b[36m(_trainable pid=136282)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=136282)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=136282)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=136282)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=137323)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=137323)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=137323)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=137323)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=137323)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=137323)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=137323)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=137323)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=137323)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_42914ad4_lightning\n",
      "\u001b[36m(_trainable pid=137323)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=137323)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=137323)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=137323)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "2024-08-06 14:41:49,502\tERROR tune_controller.py:1331 -- Trial task failed for trial _trainable_42914ad4\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/worker.py\", line 861, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=137323, ip=172.26.93.179, actor_id=8f26f513756798c08187fa8601000000, repr=_trainable)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 98, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/autotune/_manager.py\", line 439, in _trainable\n",
      "    model.train(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/model/base/_training_mixin.py\", line 143, in train\n",
      "    return runner()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainrunner.py\", line 98, in __call__\n",
      "    self.trainer.fit(self.training_plan, self.data_splitter)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainer.py\", line 219, in fit\n",
      "    super().fit(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 989, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 1035, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n",
      "    self.advance()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 137, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 285, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/utilities.py\", line 182, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 134, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 391, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 309, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 403, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainingplans.py\", line 360, in validation_step\n",
      "    _, _, scvi_loss = self.forward(batch, loss_kwargs=self.loss_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainingplans.py\", line 278, in forward\n",
      "    return self.module(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 41, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 203, in forward\n",
      "    return _generic_forward(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 739, in _generic_forward\n",
      "    inference_outputs = module.inference(**inference_inputs, **inference_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 41, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 307, in inference\n",
      "    return self._regular_inference(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 41, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/_vae.py\", line 323, in _regular_inference\n",
      "    qz, z = self.z_encoder(encoder_input, batch_index, *categorical_input)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/nn/_base_components.py\", line 282, in forward\n",
      "    dist = Normal(q_m, q_v.sqrt())\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/distributions/normal.py\", line 57, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 70, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter loc (Tensor of shape (55, 3)) of distribution Normal(loc: torch.Size([55, 3]), scale: torch.Size([55, 3])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0')\n",
      "\u001b[36m(_trainable pid=138056)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=138056)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=138056)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=138056)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=138056)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=138056)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=138056)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=138056)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=138056)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6642ffa2_lightning\n",
      "\u001b[36m(_trainable pid=138056)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=138056)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=138056)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=138056)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_de2babe8_lightning\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=138747)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=139683)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=139683)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=139683)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=139683)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=139683)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=139683)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=139683)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=139683)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=139683)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_04305e95_lightning\n",
      "\u001b[36m(_trainable pid=139683)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=139683)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=139683)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=139683)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=140651)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=140651)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=140651)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=140651)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=140651)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=140651)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=140651)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=140651)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=140651)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_da79a21a_lightning\n",
      "\u001b[36m(_trainable pid=140651)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=140651)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=140651)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=140651)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=141394)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=141394)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=141394)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=141394)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=141394)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=141394)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=141394)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=141394)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=141394)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_55a0137e_lightning\n",
      "\u001b[36m(_trainable pid=141394)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=141394)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=141394)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=141394)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=142162)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=142162)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=142162)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=142162)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=142162)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=142162)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=142162)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=142162)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=142162)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c8f972a1_lightning\n",
      "\u001b[36m(_trainable pid=142162)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=142162)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=142162)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=142162)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=142670)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=142670)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=142670)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=142670)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=142670)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=142670)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=142670)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=142670)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=142670)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_42f0da18_lightning\n",
      "\u001b[36m(_trainable pid=142670)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=142670)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=142670)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=142670)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_81b763cd_lightning\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=143317)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=143891)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=143891)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=143891)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=143891)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=143891)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=143891)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=143891)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=143891)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=143891)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b84e3fac_lightning\n",
      "\u001b[36m(_trainable pid=143891)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=143891)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=143891)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=143891)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=144482)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=144482)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=144482)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=144482)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=144482)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=144482)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=144482)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=144482)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=144482)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5c285b51_lightning\n",
      "\u001b[36m(_trainable pid=144482)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=144482)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=144482)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=144482)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=145105)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=145105)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=145105)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=145105)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=145105)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=145105)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=145105)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=145105)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=145105)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c68c5bb4_lightning\n",
      "\u001b[36m(_trainable pid=145105)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=145105)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=145105)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=145105)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=145695)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=145695)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=145695)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=145695)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=145695)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=145695)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=145695)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=145695)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=145695)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4fffdd62_lightning\n",
      "\u001b[36m(_trainable pid=145695)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=145695)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=145695)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=145695)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=146343)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=146343)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=146343)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=146343)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=146343)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=146343)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=146343)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=146343)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=146343)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_58f24c9d_lightning\n",
      "\u001b[36m(_trainable pid=146343)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=146343)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=146343)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=146343)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=146949)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=146949)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=146949)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=146949)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=146949)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=146949)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=146949)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=146949)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=146949)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_7f268957_lightning\n",
      "\u001b[36m(_trainable pid=146949)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=146949)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=146949)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=146949)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=147526)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=147526)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=147526)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=147526)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=147526)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=147526)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=147526)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=147526)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=147526)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d3501859_lightning\n",
      "\u001b[36m(_trainable pid=147526)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=147526)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=147526)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=147526)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=148112)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=148112)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=148112)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=148112)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=148112)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=148112)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=148112)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=148112)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=148112)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c88d1a69_lightning\n",
      "\u001b[36m(_trainable pid=148112)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=148112)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=148112)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=148112)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=148751)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=148751)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=148751)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=148751)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=148751)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=148751)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=148751)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=148751)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=148751)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3fc1ecda_lightning\n",
      "\u001b[36m(_trainable pid=148751)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=148751)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=148751)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=148751)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=149258)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=149258)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=149258)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=149258)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=149258)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=149258)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=149258)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=149258)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=149258)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_bcf1dd29_lightning\n",
      "\u001b[36m(_trainable pid=149258)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=149258)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=149258)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=149258)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=149828)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=149828)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=149828)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=149828)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=149828)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=149828)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=149828)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=149828)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=149828)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cd05100e_lightning\n",
      "\u001b[36m(_trainable pid=149828)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=149828)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=149828)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=149828)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=150670)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=150670)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=150670)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=150670)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=150670)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=150670)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=150670)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=150670)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=150670)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5b0d8827_lightning\n",
      "\u001b[36m(_trainable pid=150670)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=150670)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=150670)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=150670)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=151174)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=151174)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=151174)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=151174)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=151174)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=151174)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=151174)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=151174)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=151174)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_12e70e8e_lightning\n",
      "\u001b[36m(_trainable pid=151174)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=151174)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=151174)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=151174)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=151743)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=151743)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=151743)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=151743)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=151743)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=151743)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=151743)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=151743)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=151743)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6a34cbdd_lightning\n",
      "\u001b[36m(_trainable pid=151743)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=151743)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=151743)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=151743)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cdea895e_lightning\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=152457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=153027)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=153027)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=153027)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=153027)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=153027)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=153027)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=153027)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=153027)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=153027)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_525a5baf_lightning\n",
      "\u001b[36m(_trainable pid=153027)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=153027)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=153027)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=153027)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=153728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=153728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=153728)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=153728)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=153728)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=153728)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=153728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=153728)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=153728)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_94a226aa_lightning\n",
      "\u001b[36m(_trainable pid=153728)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=153728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=153728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=153728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=154537)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=154537)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=154537)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=154537)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=154537)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=154537)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=154537)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=154537)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=154537)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5968d7e3_lightning\n",
      "\u001b[36m(_trainable pid=154537)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=154537)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=154537)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=154537)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=155152)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=155152)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=155152)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=155152)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=155152)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=155152)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=155152)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=155152)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=155152)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a31a7a80_lightning\n",
      "\u001b[36m(_trainable pid=155152)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=155152)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=155152)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=155152)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=156093)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=156093)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=156093)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=156093)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=156093)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=156093)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=156093)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=156093)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=156093)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_2ab03f00_lightning\n",
      "\u001b[36m(_trainable pid=156093)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=156093)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=156093)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=156093)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=156909)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=156909)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=156909)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=156909)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=156909)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=156909)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=156909)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=156909)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=156909)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a9d158c7_lightning\n",
      "\u001b[36m(_trainable pid=156909)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=156909)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=156909)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=156909)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e6f951cf_lightning\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=157570)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=158264)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=158264)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=158264)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=158264)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=158264)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=158264)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=158264)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=158264)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=158264)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a2e30a13_lightning\n",
      "\u001b[36m(_trainable pid=158264)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=158264)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=158264)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=158264)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=158952)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=158952)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=158952)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=158952)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=158952)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=158952)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=158952)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=158952)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=158952)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9f6458ac_lightning\n",
      "\u001b[36m(_trainable pid=158952)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=158952)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=158952)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=158952)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=159972)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=159972)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=159972)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=159972)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=159972)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=159972)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=159972)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=159972)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=159972)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_70f94d74_lightning\n",
      "\u001b[36m(_trainable pid=159972)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=159972)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=159972)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=159972)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=160571)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=160571)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=160571)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=160571)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=160571)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=160571)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=160571)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=160571)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=160571)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9181de66_lightning\n",
      "\u001b[36m(_trainable pid=160571)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=160571)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=160571)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=160571)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=161493)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=161493)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=161493)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=161493)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=161493)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=161493)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=161493)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=161493)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=161493)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_60551a91_lightning\n",
      "\u001b[36m(_trainable pid=161493)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=161493)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=161493)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=161493)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=162097)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=162097)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=162097)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=162097)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=162097)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=162097)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=162097)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=162097)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=162097)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_540d9481_lightning\n",
      "\u001b[36m(_trainable pid=162097)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=162097)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=162097)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=162097)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=162928)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=162928)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=162928)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=162928)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=162928)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=162928)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=162928)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=162928)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=162928)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_64b2c2a6_lightning\n",
      "\u001b[36m(_trainable pid=162928)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=162928)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=162928)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=162928)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=163633)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=163633)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=163633)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=163633)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=163633)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=163633)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=163633)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=163633)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=163633)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_17a9660f_lightning\n",
      "\u001b[36m(_trainable pid=163633)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=163633)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=163633)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=163633)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=164573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=164573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=164573)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=164573)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=164573)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=164573)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=164573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=164573)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=164573)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_46c79ccf_lightning\n",
      "\u001b[36m(_trainable pid=164573)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=164573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=164573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=164573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=165316)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=165316)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=165316)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=165316)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=165316)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=165316)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=165316)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=165316)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=165316)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_8e38bef7_lightning\n",
      "\u001b[36m(_trainable pid=165316)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=165316)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=165316)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=165316)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=166179)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=166179)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=166179)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=166179)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=166179)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=166179)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=166179)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=166179)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=166179)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_fe6dcb92_lightning\n",
      "\u001b[36m(_trainable pid=166179)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=166179)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=166179)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=166179)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=166983)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=166983)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=166983)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=166983)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=166983)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=166983)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=166983)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=166983)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=166983)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f74758f9_lightning\n",
      "\u001b[36m(_trainable pid=166983)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=166983)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=166983)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=166983)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=167880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=167880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=167880)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=167880)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=167880)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=167880)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=167880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=167880)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=167880)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_8c1ec218_lightning\n",
      "\u001b[36m(_trainable pid=167880)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=167880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=167880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=167880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=169002)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=169002)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=169002)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=169002)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=169002)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=169002)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=169002)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=169002)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=169002)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_fae2d9fc_lightning\n",
      "\u001b[36m(_trainable pid=169002)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=169002)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=169002)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=169002)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=170192)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=170192)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=170192)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=170192)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=170192)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=170192)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=170192)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=170192)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=170192)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9b8f4d14_lightning\n",
      "\u001b[36m(_trainable pid=170192)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=170192)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=170192)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=170192)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=171024)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=171024)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=171024)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=171024)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=171024)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=171024)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=171024)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=171024)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=171024)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_203faa3b_lightning\n",
      "\u001b[36m(_trainable pid=171024)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=171024)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=171024)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=171024)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_dd843b3a_lightning\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171812)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=172552)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=172552)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=172552)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=172552)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=172552)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=172552)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=172552)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=172552)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=172552)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_47bbf74b_lightning\n",
      "\u001b[36m(_trainable pid=172552)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=172552)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=172552)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=172552)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=173421)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=173421)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=173421)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=173421)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=173421)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=173421)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=173421)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=173421)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=173421)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9ee10303_lightning\n",
      "\u001b[36m(_trainable pid=173421)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=173421)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=173421)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=173421)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=173985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=173985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=173985)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=173985)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=173985)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=173985)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=173985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=173985)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=173985)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_81f2bdac_lightning\n",
      "\u001b[36m(_trainable pid=173985)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=173985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=173985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=173985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=174537)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=174537)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=174537)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=174537)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=174537)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=174537)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=174537)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=174537)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=174537)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d40fedfa_lightning\n",
      "\u001b[36m(_trainable pid=174537)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=174537)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=174537)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=174537)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=175237)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=175237)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=175237)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=175237)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=175237)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=175237)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=175237)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=175237)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=175237)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_168d9866_lightning\n",
      "\u001b[36m(_trainable pid=175237)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=175237)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=175237)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=175237)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_2bae690d_lightning\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=176019)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=176811)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=176811)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=176811)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=176811)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=176811)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=176811)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=176811)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=176811)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=176811)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_0d1b914f_lightning\n",
      "\u001b[36m(_trainable pid=176811)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=176811)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=176811)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=176811)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=177547)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=177547)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=177547)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=177547)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=177547)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=177547)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=177547)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=177547)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=177547)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_ba29a908_lightning\n",
      "\u001b[36m(_trainable pid=177547)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=177547)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=177547)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=177547)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3acb3c02_lightning\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178313)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=179215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=179215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=179215)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=179215)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=179215)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=179215)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=179215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=179215)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=179215)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c2f25a86_lightning\n",
      "\u001b[36m(_trainable pid=179215)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=179215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=179215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=179215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=180267)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=180267)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=180267)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=180267)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=180267)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=180267)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=180267)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=180267)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=180267)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_7c98096f_lightning\n",
      "\u001b[36m(_trainable pid=180267)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=180267)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=180267)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=180267)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=180993)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=180993)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=180993)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=180993)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=180993)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=180993)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=180993)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=180993)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=180993)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a8be2ff1_lightning\n",
      "\u001b[36m(_trainable pid=180993)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=180993)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=180993)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=180993)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=181800)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=181800)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=181800)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=181800)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=181800)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=181800)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=181800)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=181800)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=181800)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_764bf9c6_lightning\n",
      "\u001b[36m(_trainable pid=181800)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=181800)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=181800)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=181800)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=182719)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=182719)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=182719)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=182719)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=182719)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=182719)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=182719)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=182719)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=182719)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3eaf1e3d_lightning\n",
      "\u001b[36m(_trainable pid=182719)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=182719)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=182719)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=182719)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=183513)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=183513)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=183513)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=183513)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=183513)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=183513)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=183513)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=183513)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=183513)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f8d350a6_lightning\n",
      "\u001b[36m(_trainable pid=183513)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=183513)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=183513)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=183513)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=184312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=184312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=184312)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=184312)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=184312)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=184312)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=184312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=184312)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=184312)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cddeba55_lightning\n",
      "\u001b[36m(_trainable pid=184312)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=184312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=184312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=184312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=184997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=184997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=184997)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=184997)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=184997)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=184997)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=184997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=184997)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=184997)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9dfee604_lightning\n",
      "\u001b[36m(_trainable pid=184997)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=184997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=184997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=184997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=185730)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=185730)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=185730)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=185730)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=185730)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=185730)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=185730)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=185730)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=185730)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_235a2392_lightning\n",
      "\u001b[36m(_trainable pid=185730)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=185730)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=185730)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=185730)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=186600)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=186600)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=186600)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=186600)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=186600)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=186600)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=186600)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=186600)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=186600)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_84a28e0a_lightning\n",
      "\u001b[36m(_trainable pid=186600)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=186600)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=186600)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=186600)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=187599)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=187599)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=187599)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=187599)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=187599)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=187599)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=187599)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=187599)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=187599)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_361eb529_lightning\n",
      "\u001b[36m(_trainable pid=187599)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=187599)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=187599)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=187599)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=188780)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=188780)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=188780)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=188780)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=188780)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=188780)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=188780)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=188780)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=188780)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_388adff2_lightning\n",
      "\u001b[36m(_trainable pid=188780)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=188780)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=188780)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=188780)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=189698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=189698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=189698)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=189698)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=189698)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=189698)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=189698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=189698)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=189698)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_13b24fcb_lightning\n",
      "\u001b[36m(_trainable pid=189698)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=189698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=189698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=189698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=190787)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=190787)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=190787)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=190787)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=190787)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=190787)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=190787)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=190787)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=190787)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a0215b9c_lightning\n",
      "\u001b[36m(_trainable pid=190787)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=190787)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=190787)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=190787)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=191863)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=191863)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=191863)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=191863)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=191863)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=191863)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=191863)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=191863)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=191863)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3c2f0161_lightning\n",
      "\u001b[36m(_trainable pid=191863)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=191863)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=191863)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=191863)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_95b7d04f_lightning\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=192877)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=193916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=193916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=193916)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=193916)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=193916)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=193916)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=193916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=193916)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=193916)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1e9d9c5c_lightning\n",
      "\u001b[36m(_trainable pid=193916)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=193916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=193916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=193916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=194885)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=194885)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=194885)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=194885)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=194885)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=194885)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=194885)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=194885)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=194885)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c4845dd5_lightning\n",
      "\u001b[36m(_trainable pid=194885)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=194885)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=194885)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=194885)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=195934)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=195934)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=195934)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=195934)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=195934)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=195934)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=195934)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=195934)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=195934)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f7206f1b_lightning\n",
      "\u001b[36m(_trainable pid=195934)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=195934)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=195934)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=195934)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=196685)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=196685)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=196685)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=196685)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=196685)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=196685)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=196685)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=196685)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=196685)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_faf145e2_lightning\n",
      "\u001b[36m(_trainable pid=196685)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=196685)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=196685)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=196685)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=197509)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=197509)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=197509)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=197509)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=197509)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=197509)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=197509)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=197509)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=197509)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4e142c0c_lightning\n",
      "\u001b[36m(_trainable pid=197509)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=197509)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=197509)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=197509)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=198637)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=198637)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=198637)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=198637)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=198637)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=198637)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=198637)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=198637)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=198637)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cafad75b_lightning\n",
      "\u001b[36m(_trainable pid=198637)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=198637)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=198637)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=198637)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=199562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=199562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=199562)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=199562)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=199562)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=199562)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=199562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=199562)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=199562)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4ed9ea80_lightning\n",
      "\u001b[36m(_trainable pid=199562)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=199562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=199562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=199562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=200620)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=200620)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=200620)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=200620)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=200620)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=200620)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=200620)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=200620)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=200620)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_0e694a6f_lightning\n",
      "\u001b[36m(_trainable pid=200620)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=200620)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=200620)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=200620)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_55379ab6_lightning\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=201778)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=202879)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=202879)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=202879)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=202879)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=202879)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=202879)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=202879)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=202879)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=202879)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_8a151750_lightning\n",
      "\u001b[36m(_trainable pid=202879)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=202879)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=202879)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=202879)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=203914)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=203914)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=203914)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=203914)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=203914)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=203914)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=203914)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=203914)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=203914)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b5b43e77_lightning\n",
      "\u001b[36m(_trainable pid=203914)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=203914)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=203914)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=203914)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=204624)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=204624)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=204624)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=204624)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=204624)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=204624)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=204624)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=204624)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=204624)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6f311686_lightning\n",
      "\u001b[36m(_trainable pid=204624)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=204624)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=204624)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=204624)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=205312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=205312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=205312)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=205312)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=205312)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=205312)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=205312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=205312)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=205312)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_52f853af_lightning\n",
      "\u001b[36m(_trainable pid=205312)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=205312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=205312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=205312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_439b51aa_lightning\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206145)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=206878)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=206878)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=206878)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=206878)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=206878)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=206878)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=206878)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=206878)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=206878)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_62511af0_lightning\n",
      "\u001b[36m(_trainable pid=206878)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=206878)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=206878)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=206878)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=207581)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=207581)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=207581)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=207581)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=207581)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=207581)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=207581)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=207581)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=207581)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_94b5362d_lightning\n",
      "\u001b[36m(_trainable pid=207581)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=207581)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=207581)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=207581)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=208275)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=208275)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=208275)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=208275)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=208275)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=208275)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=208275)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=208275)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=208275)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_19bd1722_lightning\n",
      "\u001b[36m(_trainable pid=208275)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=208275)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=208275)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=208275)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=209123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=209123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=209123)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=209123)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=209123)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=209123)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=209123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=209123)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=209123)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9f3ab7ce_lightning\n",
      "\u001b[36m(_trainable pid=209123)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=209123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=209123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=209123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=209851)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=209851)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=209851)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=209851)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=209851)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=209851)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=209851)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=209851)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=209851)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_7cafa59b_lightning\n",
      "\u001b[36m(_trainable pid=209851)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=209851)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=209851)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=209851)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=210501)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=210501)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=210501)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=210501)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=210501)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=210501)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=210501)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=210501)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=210501)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3e2e3ebb_lightning\n",
      "\u001b[36m(_trainable pid=210501)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=210501)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=210501)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=210501)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=211682)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=211682)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=211682)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=211682)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=211682)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=211682)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=211682)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=211682)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=211682)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4c2158e6_lightning\n",
      "\u001b[36m(_trainable pid=211682)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=211682)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=211682)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=211682)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4a54d32c_lightning\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=213304)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=214698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=214698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=214698)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=214698)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=214698)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=214698)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=214698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=214698)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=214698)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_72150064_lightning\n",
      "\u001b[36m(_trainable pid=214698)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=214698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=214698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=214698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=216137)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=216137)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=216137)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=216137)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=216137)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=216137)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=216137)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=216137)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=216137)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_596f114c_lightning\n",
      "\u001b[36m(_trainable pid=216137)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=216137)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=216137)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=216137)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=216960)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=216960)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=216960)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=216960)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=216960)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=216960)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=216960)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=216960)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=216960)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c61172bd_lightning\n",
      "\u001b[36m(_trainable pid=216960)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=216960)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=216960)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=216960)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=217807)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=217807)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=217807)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=217807)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=217807)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=217807)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=217807)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=217807)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=217807)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6ccbb9f4_lightning\n",
      "\u001b[36m(_trainable pid=217807)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=217807)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=217807)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=217807)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=218473)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=218473)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=218473)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=218473)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=218473)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=218473)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=218473)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=218473)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=218473)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5444df2b_lightning\n",
      "\u001b[36m(_trainable pid=218473)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=218473)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=218473)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=218473)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=219135)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=219135)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=219135)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=219135)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=219135)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=219135)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=219135)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=219135)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=219135)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_93ae7351_lightning\n",
      "\u001b[36m(_trainable pid=219135)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=219135)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=219135)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=219135)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=219856)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=219856)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=219856)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=219856)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=219856)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=219856)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=219856)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=219856)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=219856)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a23160f2_lightning\n",
      "\u001b[36m(_trainable pid=219856)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=219856)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=219856)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=219856)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=220639)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=220639)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=220639)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=220639)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=220639)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=220639)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=220639)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=220639)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=220639)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_290776cf_lightning\n",
      "\u001b[36m(_trainable pid=220639)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=220639)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=220639)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=220639)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_fdc97fbd_lightning\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=221244)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_8fe9680f_lightning\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=221961)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=222830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=222830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=222830)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=222830)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=222830)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=222830)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=222830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=222830)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=222830)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_673ad42b_lightning\n",
      "\u001b[36m(_trainable pid=222830)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=222830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=222830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=222830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_280d5b2a_lightning\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=223568)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224140)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=224140)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=224140)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=224140)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=224140)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=224140)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=224140)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=224140)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=224140)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_003a3cdd_lightning\n",
      "\u001b[36m(_trainable pid=224140)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=224140)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=224140)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=224140)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=224731)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=224731)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=224731)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=224731)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=224731)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=224731)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=224731)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=224731)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=224731)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c5f21e18_lightning\n",
      "\u001b[36m(_trainable pid=224731)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=224731)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=224731)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=224731)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=225374)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=225374)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=225374)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=225374)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=225374)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=225374)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=225374)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=225374)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=225374)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cfdeeeec_lightning\n",
      "\u001b[36m(_trainable pid=225374)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=225374)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=225374)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=225374)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=225943)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=225943)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=225943)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=225943)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=225943)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=225943)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=225943)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=225943)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=225943)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f4a6adbe_lightning\n",
      "\u001b[36m(_trainable pid=225943)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=225943)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=225943)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=225943)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=226541)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=226541)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=226541)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=226541)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=226541)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=226541)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=226541)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=226541)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=226541)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_121ab6b2_lightning\n",
      "\u001b[36m(_trainable pid=226541)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=226541)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=226541)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=226541)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_442b9c86_lightning\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227085)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227695)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=227695)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=227695)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=227695)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=227695)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=227695)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=227695)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=227695)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=227695)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_64787e16_lightning\n",
      "\u001b[36m(_trainable pid=227695)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=227695)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=227695)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=227695)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=228300)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=228300)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=228300)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=228300)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=228300)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=228300)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=228300)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=228300)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=228300)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_95cc3e7d_lightning\n",
      "\u001b[36m(_trainable pid=228300)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=228300)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=228300)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=228300)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=228846)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=228846)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=228846)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=228846)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=228846)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=228846)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=228846)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=228846)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=228846)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5d0760b9_lightning\n",
      "\u001b[36m(_trainable pid=228846)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=228846)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=228846)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=228846)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=229410)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=229410)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=229410)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=229410)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=229410)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=229410)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=229410)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=229410)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=229410)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5abb90db_lightning\n",
      "\u001b[36m(_trainable pid=229410)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=229410)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=229410)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=229410)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=230028)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=230028)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=230028)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=230028)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=230028)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=230028)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=230028)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=230028)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=230028)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_2191f7df_lightning\n",
      "\u001b[36m(_trainable pid=230028)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=230028)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=230028)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=230028)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=230575)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=230575)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=230575)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=230575)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=230575)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=230575)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=230575)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=230575)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=230575)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4c02f855_lightning\n",
      "\u001b[36m(_trainable pid=230575)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=230575)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=230575)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=230575)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=231120)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=231120)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=231120)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=231120)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=231120)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=231120)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=231120)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=231120)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=231120)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_38dd4a74_lightning\n",
      "\u001b[36m(_trainable pid=231120)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=231120)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=231120)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=231120)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=231765)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=231765)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=231765)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=231765)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=231765)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=231765)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=231765)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=231765)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=231765)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_2c12d2f4_lightning\n",
      "\u001b[36m(_trainable pid=231765)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=231765)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=231765)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=231765)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3ee321e8_lightning\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=232344)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=232914)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=232914)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=232914)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=232914)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=232914)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=232914)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=232914)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=232914)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=232914)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e6d8c579_lightning\n",
      "\u001b[36m(_trainable pid=232914)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=232914)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=232914)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=232914)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=233521)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=233521)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=233521)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=233521)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=233521)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=233521)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=233521)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=233521)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=233521)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_96a94fd3_lightning\n",
      "\u001b[36m(_trainable pid=233521)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=233521)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=233521)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=233521)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a727d39f_lightning\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m [2024-08-06 15:03:33,316 E 233999 234351] logging.cc:108: Stack trace: \n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m  /home/chzhan1/.local/lib/python3.9/site-packages/ray/_raylet.so(+0x1021b3a) [0x1508da376b3a] ray::operator<<()\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/_raylet.so(+0x10245f8) [0x1508da3795f8] ray::TerminateHandler()\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /apps/easybuild-2022/easybuild/software/Core/GCCcore/11.3.0/lib64/libstdc++.so.6(+0xb4f9a) [0x1508d91e8f9a] __cxxabiv1::__terminate()\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /apps/easybuild-2022/easybuild/software/Core/GCCcore/11.3.0/lib64/libstdc++.so.6(+0xb5005) [0x1508d91e9005]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /apps/easybuild-2022/easybuild/software/Core/GCCcore/11.3.0/lib64/libstdc++.so.6(__gxx_personality_v0+0x2bc) [0x1508d91e895c] __gxx_personality_v0\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /apps/easybuild-2022/easybuild/software/Core/GCCcore/11.3.0/lib64/libgcc_s.so.1(+0x115c4) [0x1508d912b5c4] _Unwind_ForcedUnwind_Phase2\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /apps/easybuild-2022/easybuild/software/Core/GCCcore/11.3.0/lib64/libgcc_s.so.1(_Unwind_ForcedUnwind+0x132) [0x1508d912bcb2] _Unwind_ForcedUnwind\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libc.so.6(+0x92406) [0x1508dae92406] __GI___pthread_unwind\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libc.so.6(pthread_exit+0x3a) [0x1508dae8adba] pthread_exit\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0xc9845) [0x1508db2c9845]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x614e1) [0x1508db2614e1]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(PyEval_RestoreThread+0x16) [0x1508db32e166] PyEval_RestoreThread\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so(+0x570745) [0x14d95316f745] torch::autograd::dispatch_to()\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so(+0x570a0e) [0x14d95316fa0e] torch::autograd::THPVariable_cpu()\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x11ce51) [0x1508db31ce51]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x757) [0x1508db30fe37] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x1508db30e725]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x1508db31bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x47a) [0x1508db30fb5a] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x1508db30e725]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x1508db31bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1508db3246e1]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1508db3147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x1508db30e725]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x1508db31bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1508db3246e1]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1508db3147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x1508db30e725]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x1508db31bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1508db3246e1]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1508db3147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x1508db30e725]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x1508db31bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1508db3246e1]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1508db3147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x1508db30e725]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x1508db31bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1508db3246e1]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1508db3147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x1508db30e725]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x1508db31bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1508db3246e1]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1508db3147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x1508db30e725]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x1508db31bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1508db3246e1]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1508db3147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x11c223) [0x1508db31c223]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1508db3246e1]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1508db3147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x1508db30e725]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x1508db31bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x1246e1) [0x1508db3246e1]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1508db3147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x11c223) [0x1508db31c223]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x757) [0x1508db30fe37] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x11c223) [0x1508db31c223]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x757) [0x1508db30fe37] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x1508db30e725]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x1508db31bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyEval_EvalFrameDefault+0x5105) [0x1508db3147e5] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x10e725) [0x1508db30e725]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(_PyFunction_Vectorcall+0xe5) [0x1508db31bf95] _PyFunction_Vectorcall\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m /lib64/libpython3.9.so.1.0(+0x124782) [0x1508db324782]\n",
      "\u001b[36m(_trainable pid=233999)\u001b[0m \n",
      "\u001b[36m(_trainable pid=234718)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=234718)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=234718)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=234718)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=234718)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=234718)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=234718)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=234718)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=234718)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a9d51582_lightning\n",
      "\u001b[36m(_trainable pid=234718)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=234718)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=234718)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=234718)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=235282)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=235282)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=235282)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=235282)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=235282)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=235282)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=235282)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=235282)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=235282)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_081a6724_lightning\n",
      "\u001b[36m(_trainable pid=235282)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=235282)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=235282)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=235282)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=235916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=235916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=235916)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=235916)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=235916)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=235916)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=235916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=235916)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=235916)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_bde612d0_lightning\n",
      "\u001b[36m(_trainable pid=235916)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=235916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=235916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=235916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=236543)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=236543)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=236543)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=236543)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=236543)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=236543)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=236543)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=236543)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=236543)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6ff46aa3_lightning\n",
      "\u001b[36m(_trainable pid=236543)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=236543)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=236543)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=236543)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=237017)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=237017)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=237017)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=237017)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=237017)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=237017)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=237017)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=237017)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=237017)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_53a779a4_lightning\n",
      "\u001b[36m(_trainable pid=237017)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=237017)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=237017)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=237017)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=237692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=237692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=237692)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=237692)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=237692)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=237692)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=237692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=237692)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=237692)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5be4bde0_lightning\n",
      "\u001b[36m(_trainable pid=237692)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=237692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=237692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=237692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=238257)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=238257)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=238257)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=238257)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=238257)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=238257)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=238257)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=238257)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=238257)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_0cf77d15_lightning\n",
      "\u001b[36m(_trainable pid=238257)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=238257)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=238257)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=238257)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=238819)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=238819)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=238819)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=238819)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=238819)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=238819)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=238819)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=238819)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=238819)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3d0d0194_lightning\n",
      "\u001b[36m(_trainable pid=238819)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=238819)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=238819)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=238819)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=239296)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=239296)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=239296)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=239296)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=239296)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=239296)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=239296)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=239296)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=239296)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d8cf8fd7_lightning\n",
      "\u001b[36m(_trainable pid=239296)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=239296)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=239296)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=239296)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=240609)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=240609)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=240609)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=240609)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=240609)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=240609)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=240609)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=240609)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=240609)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_046dce8c_lightning\n",
      "\u001b[36m(_trainable pid=240609)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=240609)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=240609)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=240609)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=241881)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=241881)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=241881)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=241881)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=241881)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=241881)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=241881)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=241881)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=241881)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f52a89e4_lightning\n",
      "\u001b[36m(_trainable pid=241881)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=241881)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=241881)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=241881)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=243303)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=243303)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=243303)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=243303)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=243303)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=243303)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=243303)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=243303)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=243303)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_2c3ce1c6_lightning\n",
      "\u001b[36m(_trainable pid=243303)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=243303)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=243303)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=243303)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=245093)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=245093)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=245093)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=245093)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=245093)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=245093)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=245093)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=245093)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=245093)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_31ba1cdc_lightning\n",
      "\u001b[36m(_trainable pid=245093)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=245093)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=245093)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=245093)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=245723)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=245723)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=245723)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=245723)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=245723)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=245723)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=245723)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=245723)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=245723)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cca498fd_lightning\n",
      "\u001b[36m(_trainable pid=245723)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=245723)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=245723)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=245723)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=246607)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=246607)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=246607)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=246607)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=246607)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=246607)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=246607)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=246607)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=246607)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f2db8bfc_lightning\n",
      "\u001b[36m(_trainable pid=246607)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=246607)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=246607)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=246607)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=247552)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=247552)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=247552)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=247552)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=247552)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=247552)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=247552)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=247552)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=247552)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_72f42721_lightning\n",
      "\u001b[36m(_trainable pid=247552)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=247552)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=247552)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=247552)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=248462)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=248462)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=248462)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=248462)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=248462)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=248462)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=248462)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=248462)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=248462)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_dba6ee25_lightning\n",
      "\u001b[36m(_trainable pid=248462)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=248462)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=248462)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=248462)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=249128)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=249128)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=249128)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=249128)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=249128)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=249128)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=249128)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=249128)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=249128)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5d7c7078_lightning\n",
      "\u001b[36m(_trainable pid=249128)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=249128)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=249128)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=249128)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=250290)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=250290)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=250290)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=250290)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=250290)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=250290)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=250290)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=250290)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=250290)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3bde6449_lightning\n",
      "\u001b[36m(_trainable pid=250290)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=250290)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=250290)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=250290)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=251082)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=251082)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=251082)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=251082)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=251082)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=251082)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=251082)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=251082)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=251082)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_fe6766c0_lightning\n",
      "\u001b[36m(_trainable pid=251082)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=251082)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=251082)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=251082)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=251750)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=251750)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=251750)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=251750)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=251750)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=251750)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=251750)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=251750)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=251750)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b68b8fe2_lightning\n",
      "\u001b[36m(_trainable pid=251750)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=251750)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=251750)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=251750)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=252341)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=252341)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=252341)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=252341)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=252341)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=252341)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=252341)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=252341)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=252341)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c9a3e25e_lightning\n",
      "\u001b[36m(_trainable pid=252341)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=252341)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=252341)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=252341)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=252902)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=252902)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=252902)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=252902)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=252902)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=252902)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=252902)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=252902)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=252902)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_8bfed8da_lightning\n",
      "\u001b[36m(_trainable pid=252902)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=252902)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=252902)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=252902)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=253559)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=253559)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=253559)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=253559)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=253559)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=253559)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=253559)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=253559)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=253559)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_43cf8e2f_lightning\n",
      "\u001b[36m(_trainable pid=253559)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=253559)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=253559)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=253559)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=254117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=254117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=254117)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=254117)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=254117)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=254117)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=254117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=254117)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=254117)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9f92a3d6_lightning\n",
      "\u001b[36m(_trainable pid=254117)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=254117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=254117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=254117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=254836)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=254836)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=254836)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=254836)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=254836)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=254836)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=254836)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=254836)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=254836)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_bf911a49_lightning\n",
      "\u001b[36m(_trainable pid=254836)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=254836)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=254836)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=254836)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=255577)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=255577)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=255577)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=255577)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=255577)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=255577)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=255577)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=255577)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=255577)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_37fe58b8_lightning\n",
      "\u001b[36m(_trainable pid=255577)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=255577)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=255577)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=255577)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=256115)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=256115)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=256115)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=256115)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=256115)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=256115)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=256115)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=256115)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=256115)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_fc0da409_lightning\n",
      "\u001b[36m(_trainable pid=256115)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=256115)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=256115)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=256115)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=256796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=256796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=256796)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=256796)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=256796)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=256796)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=256796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=256796)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=256796)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_33735985_lightning\n",
      "\u001b[36m(_trainable pid=256796)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=256796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=256796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=256796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=257459)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=257459)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=257459)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=257459)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=257459)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=257459)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=257459)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=257459)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=257459)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_689d1e82_lightning\n",
      "\u001b[36m(_trainable pid=257459)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=257459)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=257459)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=257459)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=258057)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=258057)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=258057)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=258057)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=258057)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=258057)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=258057)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=258057)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=258057)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4a4eeae8_lightning\n",
      "\u001b[36m(_trainable pid=258057)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=258057)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=258057)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=258057)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c7ba0c07_lightning\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=258680)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=259223)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=259223)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=259223)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=259223)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=259223)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=259223)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=259223)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=259223)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=259223)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_061ef0f3_lightning\n",
      "\u001b[36m(_trainable pid=259223)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=259223)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=259223)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=259223)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=260329)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=260329)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=260329)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=260329)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=260329)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=260329)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=260329)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=260329)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=260329)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_60422215_lightning\n",
      "\u001b[36m(_trainable pid=260329)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=260329)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=260329)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=260329)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=261262)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=261262)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=261262)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=261262)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=261262)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=261262)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=261262)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=261262)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=261262)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d65f301f_lightning\n",
      "\u001b[36m(_trainable pid=261262)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=261262)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=261262)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=261262)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e8bf4496_lightning\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262158)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=262997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=262997)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=262997)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=262997)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=262997)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=262997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=262997)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=262997)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_7de63548_lightning\n",
      "\u001b[36m(_trainable pid=262997)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=262997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=262997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=262997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cf69fc00_lightning\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=263892)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=264806)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=264806)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=264806)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=264806)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=264806)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=264806)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=264806)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=264806)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=264806)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_071d4eb8_lightning\n",
      "\u001b[36m(_trainable pid=264806)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=264806)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=264806)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=264806)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=266040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=266040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=266040)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=266040)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=266040)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=266040)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=266040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=266040)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=266040)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_59f638e0_lightning\n",
      "\u001b[36m(_trainable pid=266040)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=266040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=266040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=266040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=266985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=266985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=266985)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=266985)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=266985)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=266985)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=266985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=266985)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=266985)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c12688a1_lightning\n",
      "\u001b[36m(_trainable pid=266985)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=266985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=266985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=266985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=267947)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=267947)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=267947)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=267947)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=267947)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=267947)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=267947)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=267947)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=267947)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cc860486_lightning\n",
      "\u001b[36m(_trainable pid=267947)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=267947)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=267947)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=267947)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=268740)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=268740)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=268740)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=268740)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=268740)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=268740)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=268740)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=268740)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=268740)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_57441c13_lightning\n",
      "\u001b[36m(_trainable pid=268740)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=268740)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=268740)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=268740)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=269635)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=269635)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=269635)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=269635)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=269635)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=269635)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=269635)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=269635)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=269635)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1f3cb65d_lightning\n",
      "\u001b[36m(_trainable pid=269635)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=269635)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=269635)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=269635)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=270510)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=270510)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=270510)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=270510)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=270510)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=270510)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=270510)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=270510)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=270510)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3b597332_lightning\n",
      "\u001b[36m(_trainable pid=270510)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=270510)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=270510)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=270510)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=271409)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=271409)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=271409)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=271409)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=271409)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=271409)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=271409)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=271409)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=271409)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_8e3a9b0a_lightning\n",
      "\u001b[36m(_trainable pid=271409)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=271409)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=271409)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=271409)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=273246)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=273246)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=273246)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=273246)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=273246)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=273246)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=273246)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=273246)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=273246)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f867e3c0_lightning\n",
      "\u001b[36m(_trainable pid=273246)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=273246)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=273246)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=273246)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=274592)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=274592)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=274592)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=274592)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=274592)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=274592)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=274592)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=274592)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=274592)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_55c8d314_lightning\n",
      "\u001b[36m(_trainable pid=274592)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=274592)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=274592)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=274592)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=276097)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=276097)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=276097)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=276097)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=276097)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=276097)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=276097)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=276097)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=276097)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_dde64bb2_lightning\n",
      "\u001b[36m(_trainable pid=276097)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=276097)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=276097)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=276097)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_50b3f956_lightning\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=277533)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=278180)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=278180)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=278180)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=278180)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=278180)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=278180)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=278180)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=278180)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=278180)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_fa5f2324_lightning\n",
      "\u001b[36m(_trainable pid=278180)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=278180)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=278180)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=278180)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=278952)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=278952)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=278952)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=278952)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=278952)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=278952)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=278952)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=278952)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=278952)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_28b09926_lightning\n",
      "\u001b[36m(_trainable pid=278952)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=278952)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=278952)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=278952)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=279813)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=279813)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=279813)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=279813)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=279813)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=279813)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=279813)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=279813)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=279813)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_daa857a5_lightning\n",
      "\u001b[36m(_trainable pid=279813)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=279813)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=279813)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=279813)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=280827)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=280827)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=280827)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=280827)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=280827)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=280827)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=280827)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=280827)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=280827)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_91783d34_lightning\n",
      "\u001b[36m(_trainable pid=280827)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=280827)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=280827)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=280827)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=281844)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=281844)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=281844)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=281844)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=281844)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=281844)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=281844)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=281844)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=281844)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5c4b4f90_lightning\n",
      "\u001b[36m(_trainable pid=281844)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=281844)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=281844)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=281844)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=282663)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=282663)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=282663)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=282663)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=282663)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=282663)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=282663)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=282663)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=282663)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3271005d_lightning\n",
      "\u001b[36m(_trainable pid=282663)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=282663)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=282663)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=282663)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=283728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=283728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=283728)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=283728)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=283728)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=283728)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=283728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=283728)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=283728)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6607ec17_lightning\n",
      "\u001b[36m(_trainable pid=283728)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=283728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=283728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=283728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=284456)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=284456)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=284456)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=284456)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=284456)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=284456)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=284456)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=284456)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=284456)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4bf171b9_lightning\n",
      "\u001b[36m(_trainable pid=284456)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=284456)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=284456)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=284456)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=285201)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=285201)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=285201)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=285201)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=285201)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=285201)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=285201)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=285201)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=285201)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_94e57a88_lightning\n",
      "\u001b[36m(_trainable pid=285201)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=285201)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=285201)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=285201)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_8b99e257_lightning\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=285733)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=286342)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=286342)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=286342)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=286342)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=286342)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=286342)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=286342)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=286342)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=286342)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d69e7fa9_lightning\n",
      "\u001b[36m(_trainable pid=286342)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=286342)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=286342)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=286342)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=286941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=286941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=286941)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=286941)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=286941)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=286941)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=286941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=286941)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=286941)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_798df94f_lightning\n",
      "\u001b[36m(_trainable pid=286941)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=286941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=286941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=286941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=287638)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=287638)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=287638)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=287638)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=287638)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=287638)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=287638)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=287638)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=287638)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6671947b_lightning\n",
      "\u001b[36m(_trainable pid=287638)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=287638)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=287638)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=287638)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=288211)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=288211)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=288211)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=288211)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=288211)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=288211)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=288211)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=288211)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=288211)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_82127151_lightning\n",
      "\u001b[36m(_trainable pid=288211)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=288211)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=288211)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=288211)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=288711)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=288711)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=288711)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=288711)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=288711)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=288711)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=288711)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=288711)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=288711)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_772ab5fc_lightning\n",
      "\u001b[36m(_trainable pid=288711)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=288711)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=288711)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=288711)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=289280)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=289280)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=289280)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=289280)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=289280)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=289280)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=289280)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=289280)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=289280)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_25591637_lightning\n",
      "\u001b[36m(_trainable pid=289280)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=289280)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=289280)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=289280)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=289880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=289880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=289880)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=289880)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=289880)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=289880)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=289880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=289880)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=289880)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_77bb9e0d_lightning\n",
      "\u001b[36m(_trainable pid=289880)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=289880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=289880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=289880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=290493)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=290493)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=290493)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=290493)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=290493)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=290493)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=290493)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=290493)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=290493)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_bec16603_lightning\n",
      "\u001b[36m(_trainable pid=290493)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=290493)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=290493)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=290493)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=291063)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=291063)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=291063)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=291063)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=291063)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=291063)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=291063)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=291063)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=291063)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e362a046_lightning\n",
      "\u001b[36m(_trainable pid=291063)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=291063)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=291063)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=291063)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=291645)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=291645)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=291645)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=291645)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=291645)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=291645)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=291645)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=291645)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=291645)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_8fc322d8_lightning\n",
      "\u001b[36m(_trainable pid=291645)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=291645)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=291645)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=291645)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=292307)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=292307)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=292307)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=292307)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=292307)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=292307)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=292307)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=292307)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=292307)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3d4e755d_lightning\n",
      "\u001b[36m(_trainable pid=292307)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=292307)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=292307)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=292307)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=293220)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=293220)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=293220)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=293220)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=293220)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=293220)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=293220)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=293220)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=293220)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_ff64a531_lightning\n",
      "\u001b[36m(_trainable pid=293220)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=293220)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=293220)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=293220)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=294104)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=294104)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=294104)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=294104)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=294104)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=294104)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=294104)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=294104)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=294104)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_ea62d3ba_lightning\n",
      "\u001b[36m(_trainable pid=294104)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=294104)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=294104)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=294104)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=294822)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=294822)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=294822)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=294822)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=294822)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=294822)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=294822)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=294822)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=294822)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_52e90074_lightning\n",
      "\u001b[36m(_trainable pid=294822)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=294822)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=294822)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=294822)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=295433)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=295433)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=295433)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=295433)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=295433)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=295433)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=295433)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=295433)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=295433)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d05464da_lightning\n",
      "\u001b[36m(_trainable pid=295433)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=295433)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=295433)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=295433)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=296001)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=296001)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=296001)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=296001)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=296001)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=296001)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=296001)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=296001)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=296001)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c13f4553_lightning\n",
      "\u001b[36m(_trainable pid=296001)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=296001)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=296001)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=296001)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=296573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=296573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=296573)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=296573)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=296573)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=296573)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=296573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=296573)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=296573)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1df3e7f5_lightning\n",
      "\u001b[36m(_trainable pid=296573)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=296573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=296573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=296573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=297072)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=297072)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=297072)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=297072)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=297072)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=297072)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=297072)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=297072)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=297072)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_27ef427a_lightning\n",
      "\u001b[36m(_trainable pid=297072)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=297072)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=297072)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=297072)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=297679)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=297679)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=297679)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=297679)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=297679)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=297679)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=297679)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=297679)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=297679)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c4cea99e_lightning\n",
      "\u001b[36m(_trainable pid=297679)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=297679)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=297679)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=297679)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=298284)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=298284)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=298284)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=298284)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=298284)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=298284)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=298284)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=298284)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=298284)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_64a7052f_lightning\n",
      "\u001b[36m(_trainable pid=298284)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=298284)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=298284)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=298284)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=298895)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=298895)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=298895)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=298895)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=298895)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=298895)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=298895)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=298895)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=298895)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_60f07d0d_lightning\n",
      "\u001b[36m(_trainable pid=298895)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=298895)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=298895)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=298895)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=299531)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=299531)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=299531)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=299531)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=299531)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=299531)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=299531)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=299531)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=299531)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_bede05bd_lightning\n",
      "\u001b[36m(_trainable pid=299531)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=299531)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=299531)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=299531)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=300101)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=300101)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=300101)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=300101)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=300101)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=300101)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=300101)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=300101)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=300101)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4f099092_lightning\n",
      "\u001b[36m(_trainable pid=300101)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=300101)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=300101)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=300101)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=300796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=300796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=300796)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=300796)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=300796)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=300796)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=300796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=300796)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=300796)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1fb297be_lightning\n",
      "\u001b[36m(_trainable pid=300796)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=300796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=300796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=300796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=301371)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=301371)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=301371)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=301371)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=301371)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=301371)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=301371)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=301371)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=301371)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1f1ffaea_lightning\n",
      "\u001b[36m(_trainable pid=301371)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=301371)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=301371)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=301371)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=302607)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=302607)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=302607)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=302607)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=302607)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=302607)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=302607)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=302607)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=302607)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1a058670_lightning\n",
      "\u001b[36m(_trainable pid=302607)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=302607)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=302607)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=302607)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=2123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=2123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=2123)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=2123)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=2123)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=2123)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=2123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=2123)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=2123)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1a79ad60_lightning\n",
      "\u001b[36m(_trainable pid=2123)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=2123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=2123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=2123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=3771)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=3771)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=3771)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=3771)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=3771)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=3771)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=3771)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=3771)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=3771)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_320f981c_lightning\n",
      "\u001b[36m(_trainable pid=3771)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=3771)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=3771)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=3771)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=4532)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=4532)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=4532)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=4532)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=4532)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=4532)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=4532)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=4532)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=4532)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_44a2c90a_lightning\n",
      "\u001b[36m(_trainable pid=4532)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=4532)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=4532)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=4532)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=5160)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=5160)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=5160)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=5160)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=5160)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=5160)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=5160)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=5160)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=5160)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5901cecf_lightning\n",
      "\u001b[36m(_trainable pid=5160)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=5160)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=5160)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=5160)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=5969)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=5969)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=5969)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=5969)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=5969)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=5969)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=5969)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=5969)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=5969)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d0a5c65e_lightning\n",
      "\u001b[36m(_trainable pid=5969)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=5969)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=5969)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=5969)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=6692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=6692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=6692)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=6692)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=6692)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=6692)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=6692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=6692)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=6692)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5f8686e1_lightning\n",
      "\u001b[36m(_trainable pid=6692)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=6692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=6692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=6692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=7529)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=7529)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=7529)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=7529)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=7529)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=7529)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=7529)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=7529)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=7529)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4422468d_lightning\n",
      "\u001b[36m(_trainable pid=7529)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=7529)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=7529)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=7529)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=8288)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=8288)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=8288)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=8288)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=8288)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=8288)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=8288)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=8288)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=8288)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d40b935a_lightning\n",
      "\u001b[36m(_trainable pid=8288)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=8288)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=8288)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=8288)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=9039)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=9039)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=9039)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=9039)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=9039)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=9039)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=9039)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=9039)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=9039)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_61eef4d9_lightning\n",
      "\u001b[36m(_trainable pid=9039)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=9039)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=9039)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=9039)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=10032)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=10032)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=10032)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=10032)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=10032)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=10032)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=10032)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=10032)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=10032)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_12afb08a_lightning\n",
      "\u001b[36m(_trainable pid=10032)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=10032)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=10032)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=10032)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=10791)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=10791)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=10791)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=10791)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=10791)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=10791)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=10791)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=10791)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=10791)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_95f1f169_lightning\n",
      "\u001b[36m(_trainable pid=10791)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=10791)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=10791)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=10791)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=11450)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=11450)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=11450)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=11450)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=11450)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=11450)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=11450)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=11450)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=11450)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4bee744e_lightning\n",
      "\u001b[36m(_trainable pid=11450)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=11450)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=11450)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=11450)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9ad27f2b_lightning\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=11970)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=12654)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=12654)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=12654)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=12654)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=12654)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=12654)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=12654)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=12654)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=12654)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e9342e72_lightning\n",
      "\u001b[36m(_trainable pid=12654)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=12654)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=12654)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=12654)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3be28bbe_lightning\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13155)\u001b[0m NaN or Inf found in input tensor.\n",
      "2024-08-06 15:21:39,281\tERROR tune_controller.py:1331 -- Trial task failed for trial _trainable_3be28bbe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/worker.py\", line 861, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=13155, ip=172.26.93.179, actor_id=92f41a9aa1fa5d143f21322001000000, repr=_trainable)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 98, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/autotune/_manager.py\", line 439, in _trainable\n",
      "    model.train(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/model/base/_training_mixin.py\", line 143, in train\n",
      "    return runner()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainrunner.py\", line 98, in __call__\n",
      "    self.trainer.fit(self.training_plan, self.data_splitter)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainer.py\", line 219, in fit\n",
      "    super().fit(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 989, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 1035, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n",
      "    self.advance()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/core/module.py\", line 1291, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 230, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 117, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 484, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 89, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/optim/adam.py\", line 205, in step\n",
      "    loss = closure()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 104, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 140, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 126, in closure\n",
      "    step_output = self._step_fn()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 315, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 309, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 382, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainingplans.py\", line 344, in training_step\n",
      "    _, _, scvi_loss = self.forward(batch, loss_kwargs=self.loss_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainingplans.py\", line 278, in forward\n",
      "    return self.module(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 32, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 203, in forward\n",
      "    return _generic_forward(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 739, in _generic_forward\n",
      "    inference_outputs = module.inference(**inference_inputs, **inference_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 32, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 307, in inference\n",
      "    return self._regular_inference(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 32, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/_vae.py\", line 323, in _regular_inference\n",
      "    qz, z = self.z_encoder(encoder_input, batch_index, *categorical_input)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/nn/_base_components.py\", line 282, in forward\n",
      "    dist = Normal(q_m, q_v.sqrt())\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/distributions/normal.py\", line 57, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 70, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter loc (Tensor of shape (128, 3)) of distribution Normal(loc: torch.Size([128, 3]), scale: torch.Size([128, 3])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_68a5fbd9_lightning\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=13865)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3e2401fb_lightning\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=14503)\u001b[0m NaN or Inf found in input tensor.\n",
      "2024-08-06 15:22:00,793\tERROR tune_controller.py:1331 -- Trial task failed for trial _trainable_3e2401fb\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/worker.py\", line 861, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=14503, ip=172.26.93.179, actor_id=e649e88f70f8df19ff8c491801000000, repr=_trainable)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 98, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/autotune/_manager.py\", line 439, in _trainable\n",
      "    model.train(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/model/base/_training_mixin.py\", line 143, in train\n",
      "    return runner()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainrunner.py\", line 98, in __call__\n",
      "    self.trainer.fit(self.training_plan, self.data_splitter)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainer.py\", line 219, in fit\n",
      "    super().fit(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 989, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 1035, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n",
      "    self.advance()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/core/module.py\", line 1291, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 230, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 117, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 484, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 89, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/optim/adam.py\", line 205, in step\n",
      "    loss = closure()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 104, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 140, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 126, in closure\n",
      "    step_output = self._step_fn()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 315, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 309, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 382, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainingplans.py\", line 344, in training_step\n",
      "    _, _, scvi_loss = self.forward(batch, loss_kwargs=self.loss_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainingplans.py\", line 278, in forward\n",
      "    return self.module(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 32, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 203, in forward\n",
      "    return _generic_forward(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 739, in _generic_forward\n",
      "    inference_outputs = module.inference(**inference_inputs, **inference_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 32, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 307, in inference\n",
      "    return self._regular_inference(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 32, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/_vae.py\", line 323, in _regular_inference\n",
      "    qz, z = self.z_encoder(encoder_input, batch_index, *categorical_input)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/nn/_base_components.py\", line 282, in forward\n",
      "    dist = Normal(q_m, q_v.sqrt())\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/distributions/normal.py\", line 57, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 70, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter loc (Tensor of shape (115, 3)) of distribution Normal(loc: torch.Size([115, 3]), scale: torch.Size([115, 3])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "\u001b[36m(_trainable pid=15109)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=15109)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=15109)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=15109)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=15109)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=15109)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=15109)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=15109)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=15109)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9abf8ad8_lightning\n",
      "\u001b[36m(_trainable pid=15109)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=15109)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=15109)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=15109)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b7623149_lightning\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=15820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=16501)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=16501)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=16501)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=16501)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=16501)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=16501)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=16501)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=16501)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=16501)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a07da054_lightning\n",
      "\u001b[36m(_trainable pid=16501)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=16501)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=16501)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=16501)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=17027)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=17027)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=17027)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=17027)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=17027)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=17027)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=17027)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=17027)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=17027)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f8d93301_lightning\n",
      "\u001b[36m(_trainable pid=17027)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=17027)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=17027)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=17027)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=17598)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=17598)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=17598)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=17598)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=17598)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=17598)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=17598)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=17598)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=17598)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_331155d6_lightning\n",
      "\u001b[36m(_trainable pid=17598)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=17598)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=17598)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=17598)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_492cf418_lightning\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=18201)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=18843)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=18843)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=18843)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=18843)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=18843)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=18843)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=18843)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=18843)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=18843)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_325feff5_lightning\n",
      "\u001b[36m(_trainable pid=18843)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=18843)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=18843)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=18843)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=19351)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=19351)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=19351)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=19351)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=19351)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=19351)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=19351)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=19351)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=19351)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_dc03eaca_lightning\n",
      "\u001b[36m(_trainable pid=19351)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=19351)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=19351)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=19351)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=19923)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=19923)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=19923)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=19923)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=19923)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=19923)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=19923)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=19923)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=19923)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_038c4837_lightning\n",
      "\u001b[36m(_trainable pid=19923)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=19923)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=19923)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=19923)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=20511)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=20511)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=20511)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=20511)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=20511)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=20511)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=20511)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=20511)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=20511)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f7c35035_lightning\n",
      "\u001b[36m(_trainable pid=20511)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=20511)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=20511)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=20511)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=21123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=21123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=21123)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=21123)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=21123)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=21123)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=21123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=21123)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=21123)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_36a54c11_lightning\n",
      "\u001b[36m(_trainable pid=21123)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=21123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=21123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=21123)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3ef582fd_lightning\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=21684)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_30ab4bfa_lightning\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22218)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=22979)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=22979)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=22979)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=22979)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=22979)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=22979)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=22979)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=22979)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=22979)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_823aa34f_lightning\n",
      "\u001b[36m(_trainable pid=22979)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=22979)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=22979)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=22979)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4b22e1a5_lightning\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=23476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=24052)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=24052)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=24052)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=24052)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=24052)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=24052)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=24052)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=24052)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=24052)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_dd7ed43d_lightning\n",
      "\u001b[36m(_trainable pid=24052)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=24052)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=24052)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=24052)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=24666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=24666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=24666)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=24666)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=24666)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=24666)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=24666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=24666)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=24666)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_fe2899f3_lightning\n",
      "\u001b[36m(_trainable pid=24666)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=24666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=24666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=24666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=25164)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=25164)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=25164)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=25164)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=25164)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=25164)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=25164)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=25164)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=25164)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d70662aa_lightning\n",
      "\u001b[36m(_trainable pid=25164)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=25164)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=25164)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=25164)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3668c744_lightning\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=25741)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=26345)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=26345)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=26345)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=26345)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=26345)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=26345)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=26345)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=26345)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=26345)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1055be18_lightning\n",
      "\u001b[36m(_trainable pid=26345)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=26345)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=26345)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=26345)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=26919)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=26919)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=26919)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=26919)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=26919)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=26919)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=26919)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=26919)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=26919)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_760f6361_lightning\n",
      "\u001b[36m(_trainable pid=26919)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=26919)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=26919)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=26919)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=27420)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=27420)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=27420)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=27420)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=27420)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=27420)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=27420)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=27420)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=27420)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3565abdc_lightning\n",
      "\u001b[36m(_trainable pid=27420)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=27420)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=27420)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=27420)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=28014)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=28014)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=28014)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=28014)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=28014)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=28014)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=28014)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=28014)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=28014)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cdb4f30e_lightning\n",
      "\u001b[36m(_trainable pid=28014)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=28014)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=28014)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=28014)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=28826)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=28826)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=28826)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=28826)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=28826)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=28826)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=28826)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=28826)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=28826)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_11f1f67d_lightning\n",
      "\u001b[36m(_trainable pid=28826)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=28826)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=28826)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=28826)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=30142)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=30142)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=30142)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=30142)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=30142)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=30142)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=30142)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=30142)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=30142)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cde7990d_lightning\n",
      "\u001b[36m(_trainable pid=30142)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=30142)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=30142)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=30142)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=31471)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=31471)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=31471)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=31471)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=31471)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=31471)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=31471)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=31471)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=31471)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d5dcbc54_lightning\n",
      "\u001b[36m(_trainable pid=31471)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=31471)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=31471)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=31471)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=33189)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=33189)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=33189)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=33189)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=33189)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=33189)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=33189)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=33189)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=33189)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e82721f0_lightning\n",
      "\u001b[36m(_trainable pid=33189)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=33189)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=33189)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=33189)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=34108)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=34108)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=34108)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=34108)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=34108)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=34108)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=34108)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=34108)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=34108)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_38a5715b_lightning\n",
      "\u001b[36m(_trainable pid=34108)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=34108)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=34108)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=34108)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=34945)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=34945)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=34945)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=34945)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=34945)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=34945)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=34945)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=34945)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=34945)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b69e64a0_lightning\n",
      "\u001b[36m(_trainable pid=34945)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=34945)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=34945)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=34945)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=35829)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=35829)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=35829)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=35829)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=35829)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=35829)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=35829)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=35829)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=35829)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_39f41110_lightning\n",
      "\u001b[36m(_trainable pid=35829)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=35829)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=35829)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=35829)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=37162)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=37162)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=37162)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=37162)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=37162)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=37162)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=37162)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=37162)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=37162)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3bf9f25b_lightning\n",
      "\u001b[36m(_trainable pid=37162)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=37162)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=37162)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=37162)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=37983)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=37983)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=37983)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=37983)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=37983)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=37983)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=37983)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=37983)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=37983)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_170b9b9c_lightning\n",
      "\u001b[36m(_trainable pid=37983)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=37983)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=37983)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=37983)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=38995)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=38995)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=38995)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=38995)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=38995)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=38995)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=38995)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=38995)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=38995)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_014193bf_lightning\n",
      "\u001b[36m(_trainable pid=38995)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=38995)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=38995)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=38995)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=40076)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=40076)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=40076)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=40076)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=40076)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=40076)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=40076)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=40076)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=40076)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a2bc3940_lightning\n",
      "\u001b[36m(_trainable pid=40076)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=40076)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=40076)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=40076)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=41035)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=41035)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=41035)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=41035)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=41035)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=41035)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=41035)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=41035)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=41035)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_39816b89_lightning\n",
      "\u001b[36m(_trainable pid=41035)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=41035)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=41035)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=41035)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=41963)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=41963)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=41963)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=41963)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=41963)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=41963)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=41963)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=41963)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=41963)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c0b0bc2e_lightning\n",
      "\u001b[36m(_trainable pid=41963)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=41963)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=41963)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=41963)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=42969)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=42969)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=42969)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=42969)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=42969)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=42969)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=42969)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=42969)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=42969)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6f1bee0e_lightning\n",
      "\u001b[36m(_trainable pid=42969)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=42969)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=42969)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=42969)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=43975)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=43975)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=43975)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=43975)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=43975)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=43975)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=43975)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=43975)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=43975)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_11b25f3b_lightning\n",
      "\u001b[36m(_trainable pid=43975)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=43975)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=43975)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=43975)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=44830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=44830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=44830)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=44830)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=44830)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=44830)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=44830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=44830)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=44830)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f6372074_lightning\n",
      "\u001b[36m(_trainable pid=44830)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=44830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=44830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=44830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45884)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=45884)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=45884)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=45884)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=45884)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=45884)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=45884)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=45884)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=45884)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f8d1528d_lightning\n",
      "\u001b[36m(_trainable pid=45884)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=45884)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=45884)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=45884)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=46789)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=46789)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=46789)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=46789)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=46789)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=46789)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=46789)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=46789)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=46789)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5c6e55f7_lightning\n",
      "\u001b[36m(_trainable pid=46789)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=46789)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=46789)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=46789)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=47529)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=47529)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=47529)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=47529)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=47529)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=47529)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=47529)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=47529)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=47529)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e453ea45_lightning\n",
      "\u001b[36m(_trainable pid=47529)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=47529)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=47529)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=47529)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=48152)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=48152)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=48152)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=48152)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=48152)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=48152)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=48152)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=48152)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=48152)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d04c031d_lightning\n",
      "\u001b[36m(_trainable pid=48152)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=48152)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=48152)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=48152)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=48917)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=48917)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=48917)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=48917)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=48917)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=48917)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=48917)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=48917)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=48917)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_477b7b67_lightning\n",
      "\u001b[36m(_trainable pid=48917)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=48917)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=48917)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=48917)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=49674)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=49674)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=49674)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=49674)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=49674)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=49674)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=49674)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=49674)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=49674)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3f0144eb_lightning\n",
      "\u001b[36m(_trainable pid=49674)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=49674)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=49674)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=49674)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=50306)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=50306)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=50306)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=50306)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=50306)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=50306)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=50306)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=50306)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=50306)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_33afd60e_lightning\n",
      "\u001b[36m(_trainable pid=50306)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=50306)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=50306)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=50306)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=51075)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=51075)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=51075)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=51075)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=51075)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=51075)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=51075)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=51075)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=51075)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_45390e37_lightning\n",
      "\u001b[36m(_trainable pid=51075)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=51075)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=51075)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=51075)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=51986)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=51986)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=51986)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=51986)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=51986)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=51986)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=51986)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=51986)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=51986)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1ffe0197_lightning\n",
      "\u001b[36m(_trainable pid=51986)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=51986)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=51986)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=51986)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=52615)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=52615)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=52615)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=52615)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=52615)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=52615)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=52615)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=52615)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=52615)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_34b63841_lightning\n",
      "\u001b[36m(_trainable pid=52615)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=52615)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=52615)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=52615)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=53509)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=53509)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=53509)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=53509)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=53509)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=53509)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=53509)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=53509)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=53509)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_0b39ebcb_lightning\n",
      "\u001b[36m(_trainable pid=53509)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=53509)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=53509)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=53509)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=54336)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=54336)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=54336)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=54336)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=54336)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=54336)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=54336)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=54336)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=54336)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c4b9712d_lightning\n",
      "\u001b[36m(_trainable pid=54336)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=54336)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=54336)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=54336)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_361a9dbb_lightning\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=54910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=55526)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=55526)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=55526)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=55526)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=55526)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=55526)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=55526)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=55526)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=55526)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f433efe1_lightning\n",
      "\u001b[36m(_trainable pid=55526)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=55526)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=55526)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=55526)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=56238)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=56238)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=56238)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=56238)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=56238)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=56238)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=56238)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=56238)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=56238)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b5b29b44_lightning\n",
      "\u001b[36m(_trainable pid=56238)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=56238)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=56238)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=56238)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b1cf9cba_lightning\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=56820)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4d1b1e76_lightning\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=57476)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=58122)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=58122)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=58122)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=58122)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=58122)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=58122)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=58122)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=58122)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=58122)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_04fe3488_lightning\n",
      "\u001b[36m(_trainable pid=58122)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=58122)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=58122)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=58122)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=58724)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=58724)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=58724)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=58724)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=58724)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=58724)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=58724)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=58724)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=58724)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_baefefc1_lightning\n",
      "\u001b[36m(_trainable pid=58724)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=58724)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=58724)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=58724)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=59299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=59299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=59299)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=59299)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=59299)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=59299)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=59299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=59299)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=59299)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_7182a3ea_lightning\n",
      "\u001b[36m(_trainable pid=59299)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=59299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=59299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=59299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=59825)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=59825)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=59825)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=59825)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=59825)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=59825)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=59825)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=59825)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=59825)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e788d540_lightning\n",
      "\u001b[36m(_trainable pid=59825)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=59825)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=59825)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=59825)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=60384)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=60384)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=60384)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=60384)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=60384)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=60384)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=60384)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=60384)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=60384)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_2424df9f_lightning\n",
      "\u001b[36m(_trainable pid=60384)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=60384)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=60384)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=60384)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=60933)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=60933)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=60933)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=60933)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=60933)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=60933)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=60933)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=60933)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=60933)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b7909e62_lightning\n",
      "\u001b[36m(_trainable pid=60933)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=60933)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=60933)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=60933)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61455)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=61455)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=61455)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=61455)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=61455)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=61455)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=61455)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=61455)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=61455)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e1b561fb_lightning\n",
      "\u001b[36m(_trainable pid=61455)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=61455)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=61455)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=61455)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=62040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=62040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=62040)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=62040)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=62040)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=62040)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=62040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=62040)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=62040)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e79961cf_lightning\n",
      "\u001b[36m(_trainable pid=62040)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=62040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=62040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=62040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b11f5ece_lightning\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=62785)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=63354)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=63354)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=63354)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=63354)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=63354)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=63354)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=63354)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=63354)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=63354)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_ae588e35_lightning\n",
      "\u001b[36m(_trainable pid=63354)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=63354)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=63354)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=63354)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=63964)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=63964)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=63964)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=63964)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=63964)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=63964)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=63964)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=63964)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=63964)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4862239e_lightning\n",
      "\u001b[36m(_trainable pid=63964)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=63964)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=63964)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=63964)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=64533)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=64533)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=64533)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=64533)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=64533)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=64533)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=64533)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=64533)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=64533)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_2dacc557_lightning\n",
      "\u001b[36m(_trainable pid=64533)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=64533)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=64533)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=64533)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=65159)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=65159)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=65159)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=65159)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=65159)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=65159)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=65159)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=65159)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=65159)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_547666bb_lightning\n",
      "\u001b[36m(_trainable pid=65159)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=65159)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=65159)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=65159)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=65728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=65728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=65728)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=65728)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=65728)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=65728)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=65728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=65728)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=65728)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a834852f_lightning\n",
      "\u001b[36m(_trainable pid=65728)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=65728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=65728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=65728)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=66326)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=66326)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=66326)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=66326)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=66326)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=66326)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=66326)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=66326)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=66326)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_04f78ce4_lightning\n",
      "\u001b[36m(_trainable pid=66326)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=66326)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=66326)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=66326)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=66852)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=66852)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=66852)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=66852)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=66852)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=66852)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=66852)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=66852)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=66852)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e7017350_lightning\n",
      "\u001b[36m(_trainable pid=66852)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=66852)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=66852)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=66852)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=67444)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=67444)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=67444)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=67444)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=67444)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=67444)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=67444)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=67444)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=67444)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c533ad9c_lightning\n",
      "\u001b[36m(_trainable pid=67444)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=67444)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=67444)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=67444)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d7ef5a3b_lightning\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=67947)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=68597)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=68597)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=68597)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=68597)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=68597)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=68597)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=68597)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=68597)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=68597)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6a3707aa_lightning\n",
      "\u001b[36m(_trainable pid=68597)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=68597)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=68597)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=68597)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_69315635_lightning\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=69165)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=69758)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=69758)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=69758)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=69758)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=69758)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=69758)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=69758)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=69758)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=69758)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_95091c87_lightning\n",
      "\u001b[36m(_trainable pid=69758)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=69758)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=69758)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=69758)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=70375)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=70375)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=70375)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=70375)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=70375)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=70375)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=70375)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=70375)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=70375)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_7f52386a_lightning\n",
      "\u001b[36m(_trainable pid=70375)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=70375)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=70375)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=70375)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=70957)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=70957)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=70957)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=70957)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=70957)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=70957)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=70957)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=70957)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=70957)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_78f1661c_lightning\n",
      "\u001b[36m(_trainable pid=70957)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=70957)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=70957)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=70957)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=71460)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=71460)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=71460)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=71460)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=71460)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=71460)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=71460)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=71460)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=71460)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1076be53_lightning\n",
      "\u001b[36m(_trainable pid=71460)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=71460)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=71460)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=71460)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=72030)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=72030)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=72030)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=72030)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=72030)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=72030)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=72030)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=72030)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=72030)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b562f361_lightning\n",
      "\u001b[36m(_trainable pid=72030)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=72030)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=72030)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=72030)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=72675)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=72675)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=72675)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=72675)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=72675)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=72675)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=72675)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=72675)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=72675)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_166e08d3_lightning\n",
      "\u001b[36m(_trainable pid=72675)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=72675)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=72675)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=72675)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=73377)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=73377)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=73377)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=73377)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=73377)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=73377)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=73377)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=73377)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=73377)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_2e80d692_lightning\n",
      "\u001b[36m(_trainable pid=73377)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=73377)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=73377)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=73377)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=74698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=74698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=74698)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=74698)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=74698)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=74698)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=74698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=74698)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=74698)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_035ded50_lightning\n",
      "\u001b[36m(_trainable pid=74698)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=74698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=74698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=74698)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=75725)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=75725)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=75725)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=75725)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=75725)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=75725)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=75725)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=75725)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=75725)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3d1163fe_lightning\n",
      "\u001b[36m(_trainable pid=75725)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=75725)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=75725)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=75725)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=76845)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=76845)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=76845)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=76845)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=76845)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=76845)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=76845)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=76845)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=76845)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_dd1f7191_lightning\n",
      "\u001b[36m(_trainable pid=76845)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=76845)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=76845)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=76845)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=78163)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=78163)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=78163)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=78163)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=78163)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=78163)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=78163)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=78163)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=78163)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5284a815_lightning\n",
      "\u001b[36m(_trainable pid=78163)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=78163)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=78163)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=78163)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_0f51bc8e_lightning\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=79653)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=81025)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=81025)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=81025)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=81025)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=81025)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=81025)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=81025)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=81025)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=81025)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_90c7b294_lightning\n",
      "\u001b[36m(_trainable pid=81025)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=81025)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=81025)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=81025)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=82154)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=82154)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=82154)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=82154)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=82154)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=82154)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=82154)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=82154)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=82154)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_88ae4aec_lightning\n",
      "\u001b[36m(_trainable pid=82154)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=82154)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=82154)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=82154)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=83268)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=83268)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=83268)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=83268)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=83268)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=83268)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=83268)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=83268)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=83268)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a4a9cca6_lightning\n",
      "\u001b[36m(_trainable pid=83268)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=83268)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=83268)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=83268)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=84173)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=84173)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=84173)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=84173)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=84173)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=84173)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=84173)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=84173)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=84173)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_dee43c28_lightning\n",
      "\u001b[36m(_trainable pid=84173)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=84173)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=84173)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=84173)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=84726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=84726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=84726)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=84726)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=84726)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=84726)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=84726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=84726)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=84726)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_ede52f93_lightning\n",
      "\u001b[36m(_trainable pid=84726)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=84726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=84726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=84726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=85490)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=85490)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=85490)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=85490)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=85490)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=85490)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=85490)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=85490)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=85490)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3c35c3b4_lightning\n",
      "\u001b[36m(_trainable pid=85490)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=85490)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=85490)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=85490)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=86215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=86215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=86215)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=86215)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=86215)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=86215)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=86215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=86215)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=86215)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_2ad53c39_lightning\n",
      "\u001b[36m(_trainable pid=86215)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=86215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=86215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=86215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4d2fd4b6_lightning\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87059)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=87939)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=87939)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=87939)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=87939)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=87939)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=87939)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=87939)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=87939)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=87939)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_7baaf329_lightning\n",
      "\u001b[36m(_trainable pid=87939)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=87939)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=87939)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=87939)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=89299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=89299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=89299)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=89299)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=89299)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=89299)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=89299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=89299)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=89299)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cb248000_lightning\n",
      "\u001b[36m(_trainable pid=89299)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=89299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=89299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=89299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_0d6ac7a3_lightning\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=90119)\u001b[0m NaN or Inf found in input tensor.\n",
      "2024-08-06 15:38:27,604\tERROR tune_controller.py:1331 -- Trial task failed for trial _trainable_0d6ac7a3\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/worker.py\", line 861, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=90119, ip=172.26.93.179, actor_id=836b33ff14bb3c82f198628701000000, repr=_trainable)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 98, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/autotune/_manager.py\", line 439, in _trainable\n",
      "    model.train(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/model/base/_training_mixin.py\", line 143, in train\n",
      "    return runner()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainrunner.py\", line 98, in __call__\n",
      "    self.trainer.fit(self.training_plan, self.data_splitter)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainer.py\", line 219, in fit\n",
      "    super().fit(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 989, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 1035, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n",
      "    self.advance()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/core/module.py\", line 1291, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 230, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 117, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 484, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 89, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/optim/adam.py\", line 205, in step\n",
      "    loss = closure()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 104, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 140, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 126, in closure\n",
      "    step_output = self._step_fn()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 315, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 309, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 382, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainingplans.py\", line 344, in training_step\n",
      "    _, _, scvi_loss = self.forward(batch, loss_kwargs=self.loss_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainingplans.py\", line 278, in forward\n",
      "    return self.module(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 32, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 203, in forward\n",
      "    return _generic_forward(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 739, in _generic_forward\n",
      "    inference_outputs = module.inference(**inference_inputs, **inference_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 32, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 307, in inference\n",
      "    return self._regular_inference(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 32, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/_vae.py\", line 323, in _regular_inference\n",
      "    qz, z = self.z_encoder(encoder_input, batch_index, *categorical_input)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/nn/_base_components.py\", line 282, in forward\n",
      "    dist = Normal(q_m, q_v.sqrt())\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/distributions/normal.py\", line 57, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 70, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter loc (Tensor of shape (128, 3)) of distribution Normal(loc: torch.Size([128, 3]), scale: torch.Size([128, 3])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "\u001b[36m(_trainable pid=90773)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=90773)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=90773)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=90773)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=90773)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=90773)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=90773)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=90773)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=90773)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1a5c785a_lightning\n",
      "\u001b[36m(_trainable pid=90773)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=90773)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=90773)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=90773)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=91372)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=91372)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=91372)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=91372)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=91372)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=91372)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=91372)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=91372)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=91372)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_ae8de6a0_lightning\n",
      "\u001b[36m(_trainable pid=91372)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=91372)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=91372)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=91372)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b4570d55_lightning\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=91870)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=92464)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=92464)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=92464)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=92464)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=92464)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=92464)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=92464)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=92464)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=92464)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_01dd7e2a_lightning\n",
      "\u001b[36m(_trainable pid=92464)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=92464)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=92464)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=92464)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=93106)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=93106)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=93106)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=93106)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=93106)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=93106)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=93106)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=93106)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=93106)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_33405085_lightning\n",
      "\u001b[36m(_trainable pid=93106)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=93106)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=93106)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=93106)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=93621)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=93621)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=93621)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=93621)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=93621)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=93621)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=93621)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=93621)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=93621)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6dfc57f4_lightning\n",
      "\u001b[36m(_trainable pid=93621)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=93621)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=93621)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=93621)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=94215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=94215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=94215)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=94215)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=94215)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=94215)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=94215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=94215)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=94215)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a754390c_lightning\n",
      "\u001b[36m(_trainable pid=94215)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=94215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=94215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=94215)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=94826)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=94826)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=94826)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=94826)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=94826)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=94826)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=94826)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=94826)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=94826)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a993030b_lightning\n",
      "\u001b[36m(_trainable pid=94826)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=94826)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=94826)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=94826)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e0da29db_lightning\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=95402)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=96058)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=96058)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=96058)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=96058)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=96058)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=96058)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=96058)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=96058)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=96058)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_384c7b22_lightning\n",
      "\u001b[36m(_trainable pid=96058)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=96058)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=96058)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=96058)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=97008)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=97008)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=97008)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=97008)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=97008)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=97008)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=97008)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=97008)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=97008)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6699c942_lightning\n",
      "\u001b[36m(_trainable pid=97008)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=97008)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=97008)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=97008)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=97672)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=97672)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=97672)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=97672)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=97672)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=97672)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=97672)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=97672)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=97672)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_031afc3f_lightning\n",
      "\u001b[36m(_trainable pid=97672)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=97672)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=97672)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=97672)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=98504)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=98504)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=98504)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=98504)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=98504)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=98504)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=98504)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=98504)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=98504)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_0260545f_lightning\n",
      "\u001b[36m(_trainable pid=98504)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=98504)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=98504)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=98504)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=99542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=99542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=99542)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=99542)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=99542)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=99542)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=99542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=99542)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=99542)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f10a81a4_lightning\n",
      "\u001b[36m(_trainable pid=99542)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=99542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=99542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=99542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=100323)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=100323)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=100323)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=100323)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=100323)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=100323)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=100323)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=100323)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=100323)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1f0e0346_lightning\n",
      "\u001b[36m(_trainable pid=100323)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=100323)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=100323)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=100323)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=101000)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=101000)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=101000)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=101000)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=101000)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=101000)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=101000)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=101000)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=101000)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d1bb306a_lightning\n",
      "\u001b[36m(_trainable pid=101000)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=101000)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=101000)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=101000)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=101760)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=101760)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=101760)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=101760)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=101760)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=101760)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=101760)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=101760)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=101760)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c83a26eb_lightning\n",
      "\u001b[36m(_trainable pid=101760)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=101760)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=101760)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=101760)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=102562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=102562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=102562)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=102562)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=102562)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=102562)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=102562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=102562)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=102562)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d9c37122_lightning\n",
      "\u001b[36m(_trainable pid=102562)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=102562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=102562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=102562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=103224)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=103224)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=103224)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=103224)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=103224)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=103224)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=103224)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=103224)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=103224)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e22fc021_lightning\n",
      "\u001b[36m(_trainable pid=103224)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=103224)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=103224)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=103224)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=103830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=103830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=103830)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=103830)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=103830)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=103830)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=103830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=103830)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=103830)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_7ab610bc_lightning\n",
      "\u001b[36m(_trainable pid=103830)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=103830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=103830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=103830)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=104666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=104666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=104666)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=104666)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=104666)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=104666)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=104666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=104666)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=104666)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_783e60cc_lightning\n",
      "\u001b[36m(_trainable pid=104666)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=104666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=104666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=104666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=105231)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=105231)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=105231)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=105231)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=105231)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=105231)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=105231)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=105231)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=105231)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_ed4dbe0f_lightning\n",
      "\u001b[36m(_trainable pid=105231)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=105231)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=105231)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=105231)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=105916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=105916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=105916)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=105916)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=105916)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=105916)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=105916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=105916)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=105916)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_8e5cee4a_lightning\n",
      "\u001b[36m(_trainable pid=105916)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=105916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=105916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=105916)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=107098)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=107098)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=107098)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=107098)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=107098)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=107098)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=107098)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=107098)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=107098)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3b9bb75f_lightning\n",
      "\u001b[36m(_trainable pid=107098)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=107098)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=107098)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=107098)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=107909)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=107909)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=107909)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=107909)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=107909)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=107909)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=107909)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=107909)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=107909)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3015d606_lightning\n",
      "\u001b[36m(_trainable pid=107909)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=107909)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=107909)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=107909)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=109088)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=109088)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=109088)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=109088)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=109088)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=109088)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=109088)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=109088)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=109088)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cd3bfd9b_lightning\n",
      "\u001b[36m(_trainable pid=109088)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=109088)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=109088)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=109088)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=109938)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=109938)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=109938)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=109938)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=109938)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=109938)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=109938)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=109938)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=109938)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_ca096bce_lightning\n",
      "\u001b[36m(_trainable pid=109938)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=109938)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=109938)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=109938)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=110726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=110726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=110726)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=110726)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=110726)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=110726)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=110726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=110726)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=110726)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c0002f99_lightning\n",
      "\u001b[36m(_trainable pid=110726)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=110726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=110726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=110726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_33131460_lightning\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=111801)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=112726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=112726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=112726)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=112726)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=112726)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=112726)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=112726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=112726)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=112726)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9fd3ef85_lightning\n",
      "\u001b[36m(_trainable pid=112726)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=112726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=112726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=112726)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=113824)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=113824)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=113824)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=113824)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=113824)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=113824)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=113824)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=113824)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=113824)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_75e52be9_lightning\n",
      "\u001b[36m(_trainable pid=113824)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=113824)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=113824)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=113824)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=114535)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=114535)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=114535)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=114535)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=114535)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=114535)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=114535)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=114535)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=114535)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_543f984b_lightning\n",
      "\u001b[36m(_trainable pid=114535)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=114535)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=114535)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=114535)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=115458)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=115458)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=115458)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=115458)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=115458)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=115458)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=115458)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=115458)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=115458)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d8856c9a_lightning\n",
      "\u001b[36m(_trainable pid=115458)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=115458)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=115458)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=115458)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=116299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=116299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=116299)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=116299)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=116299)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=116299)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=116299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=116299)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=116299)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_08b5af65_lightning\n",
      "\u001b[36m(_trainable pid=116299)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=116299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=116299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=116299)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_bfeccf2b_lightning\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=117117)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=118040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=118040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=118040)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=118040)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=118040)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=118040)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=118040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=118040)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=118040)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9907bef2_lightning\n",
      "\u001b[36m(_trainable pid=118040)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=118040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=118040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=118040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=118953)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=118953)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=118953)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=118953)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=118953)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=118953)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=118953)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=118953)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=118953)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_246f9e38_lightning\n",
      "\u001b[36m(_trainable pid=118953)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=118953)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=118953)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=118953)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=119625)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=119625)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=119625)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=119625)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=119625)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=119625)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=119625)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=119625)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=119625)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_686fb0ce_lightning\n",
      "\u001b[36m(_trainable pid=119625)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=119625)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=119625)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=119625)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=120524)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=120524)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=120524)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=120524)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=120524)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=120524)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=120524)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=120524)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=120524)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1c9cbf73_lightning\n",
      "\u001b[36m(_trainable pid=120524)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=120524)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=120524)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=120524)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=121312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=121312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=121312)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=121312)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=121312)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=121312)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=121312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=121312)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=121312)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_610e8926_lightning\n",
      "\u001b[36m(_trainable pid=121312)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=121312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=121312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=121312)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=122176)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=122176)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=122176)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=122176)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=122176)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=122176)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=122176)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=122176)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=122176)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_580f5822_lightning\n",
      "\u001b[36m(_trainable pid=122176)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=122176)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=122176)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=122176)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=123048)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=123048)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=123048)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=123048)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=123048)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=123048)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=123048)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=123048)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=123048)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_2cd1e0dc_lightning\n",
      "\u001b[36m(_trainable pid=123048)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=123048)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=123048)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=123048)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_db61b9ee_lightning\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=123928)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_00375873_lightning\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=124818)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=126050)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=126050)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=126050)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=126050)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=126050)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=126050)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=126050)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=126050)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=126050)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4192af56_lightning\n",
      "\u001b[36m(_trainable pid=126050)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=126050)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=126050)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=126050)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_fad5e670_lightning\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=127050)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=128832)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=128832)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=128832)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=128832)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=128832)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=128832)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=128832)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=128832)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=128832)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_8f23fcfe_lightning\n",
      "\u001b[36m(_trainable pid=128832)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=128832)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=128832)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=128832)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=130156)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=130156)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=130156)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=130156)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=130156)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=130156)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=130156)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=130156)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=130156)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_960f36ff_lightning\n",
      "\u001b[36m(_trainable pid=130156)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=130156)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=130156)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=130156)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=131269)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=131269)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=131269)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=131269)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=131269)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=131269)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=131269)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=131269)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=131269)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b5da473d_lightning\n",
      "\u001b[36m(_trainable pid=131269)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=131269)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=131269)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=131269)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=132294)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=132294)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=132294)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=132294)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=132294)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=132294)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=132294)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=132294)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=132294)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5c9be09a_lightning\n",
      "\u001b[36m(_trainable pid=132294)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=132294)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=132294)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=132294)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_ae2f011b_lightning\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=133342)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=134283)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=134283)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=134283)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=134283)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=134283)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=134283)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=134283)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=134283)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=134283)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_99f87c83_lightning\n",
      "\u001b[36m(_trainable pid=134283)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=134283)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=134283)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=134283)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d48ae887_lightning\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=135482)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_25d3258c_lightning\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=136692)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=137527)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=137527)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=137527)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=137527)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=137527)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=137527)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=137527)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=137527)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=137527)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9e45e406_lightning\n",
      "\u001b[36m(_trainable pid=137527)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=137527)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=137527)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=137527)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=138242)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=138242)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=138242)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=138242)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=138242)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=138242)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=138242)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=138242)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=138242)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9399f221_lightning\n",
      "\u001b[36m(_trainable pid=138242)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=138242)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=138242)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=138242)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=138930)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=138930)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=138930)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=138930)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=138930)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=138930)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=138930)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=138930)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=138930)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9e6ff07a_lightning\n",
      "\u001b[36m(_trainable pid=138930)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=138930)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=138930)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=138930)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=139628)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=139628)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=139628)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=139628)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=139628)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=139628)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=139628)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=139628)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=139628)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d4fdbd86_lightning\n",
      "\u001b[36m(_trainable pid=139628)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=139628)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=139628)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=139628)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c62fae7e_lightning\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=140327)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=141117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=141117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=141117)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=141117)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=141117)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=141117)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=141117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=141117)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=141117)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_66f793e4_lightning\n",
      "\u001b[36m(_trainable pid=141117)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=141117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=141117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=141117)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_fb0bc5ca_lightning\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=141768)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_fdf81257_lightning\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=142535)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=143274)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=143274)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=143274)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=143274)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=143274)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=143274)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=143274)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=143274)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=143274)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b2da7923_lightning\n",
      "\u001b[36m(_trainable pid=143274)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=143274)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=143274)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=143274)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=143881)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=143881)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=143881)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=143881)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=143881)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=143881)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=143881)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=143881)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=143881)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_075c8293_lightning\n",
      "\u001b[36m(_trainable pid=143881)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=143881)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=143881)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=143881)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=144419)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=144419)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=144419)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=144419)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=144419)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=144419)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=144419)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=144419)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=144419)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_53d4beac_lightning\n",
      "\u001b[36m(_trainable pid=144419)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=144419)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=144419)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=144419)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=144977)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=144977)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=144977)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=144977)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=144977)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=144977)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=144977)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=144977)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=144977)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b40f6389_lightning\n",
      "\u001b[36m(_trainable pid=144977)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=144977)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=144977)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=144977)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=145476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=145476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=145476)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=145476)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=145476)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=145476)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=145476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=145476)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=145476)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_16dba87b_lightning\n",
      "\u001b[36m(_trainable pid=145476)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=145476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=145476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=145476)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=146046)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=146046)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=146046)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=146046)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=146046)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=146046)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=146046)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=146046)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=146046)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_95cf8a64_lightning\n",
      "\u001b[36m(_trainable pid=146046)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=146046)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=146046)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=146046)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_dc69cd0d_lightning\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=146650)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=147326)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=147326)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=147326)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=147326)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=147326)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=147326)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=147326)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=147326)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=147326)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6533d969_lightning\n",
      "\u001b[36m(_trainable pid=147326)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=147326)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=147326)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=147326)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=147898)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=147898)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=147898)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=147898)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=147898)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=147898)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=147898)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=147898)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=147898)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_0385c7ea_lightning\n",
      "\u001b[36m(_trainable pid=147898)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=147898)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=147898)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=147898)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d8385af2_lightning\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=148673)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=149233)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=149233)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=149233)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=149233)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=149233)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=149233)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=149233)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=149233)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=149233)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_01f6d229_lightning\n",
      "\u001b[36m(_trainable pid=149233)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=149233)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=149233)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=149233)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=149884)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=149884)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=149884)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=149884)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=149884)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=149884)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=149884)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=149884)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=149884)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5f04ddbb_lightning\n",
      "\u001b[36m(_trainable pid=149884)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=149884)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=149884)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=149884)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=150452)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=150452)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=150452)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=150452)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=150452)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=150452)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=150452)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=150452)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=150452)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_0346e3e4_lightning\n",
      "\u001b[36m(_trainable pid=150452)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=150452)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=150452)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=150452)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=151094)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=151094)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=151094)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=151094)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=151094)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=151094)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=151094)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=151094)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=151094)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_7f01dd5a_lightning\n",
      "\u001b[36m(_trainable pid=151094)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=151094)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=151094)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=151094)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=151746)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=151746)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=151746)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=151746)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=151746)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=151746)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=151746)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=151746)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=151746)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b9c64a1b_lightning\n",
      "\u001b[36m(_trainable pid=151746)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=151746)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=151746)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=151746)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=152346)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=152346)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=152346)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=152346)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=152346)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=152346)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=152346)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=152346)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=152346)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_7bb08ba1_lightning\n",
      "\u001b[36m(_trainable pid=152346)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=152346)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=152346)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=152346)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=152997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=152997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=152997)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=152997)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=152997)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=152997)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=152997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=152997)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=152997)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_84fedb35_lightning\n",
      "\u001b[36m(_trainable pid=152997)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=152997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=152997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=152997)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=153648)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=153648)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=153648)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=153648)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=153648)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=153648)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=153648)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=153648)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=153648)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6cbcc8f4_lightning\n",
      "\u001b[36m(_trainable pid=153648)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=153648)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=153648)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=153648)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=154324)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=154324)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=154324)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=154324)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=154324)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=154324)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=154324)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=154324)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=154324)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_dc10f258_lightning\n",
      "\u001b[36m(_trainable pid=154324)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=154324)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=154324)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=154324)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=154951)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=154951)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=154951)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=154951)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=154951)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=154951)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=154951)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=154951)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=154951)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_94ed09ae_lightning\n",
      "\u001b[36m(_trainable pid=154951)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=154951)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=154951)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=154951)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=155678)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=155678)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=155678)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=155678)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=155678)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=155678)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=155678)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=155678)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=155678)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4444b981_lightning\n",
      "\u001b[36m(_trainable pid=155678)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=155678)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=155678)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=155678)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=156153)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=156153)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=156153)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=156153)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=156153)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=156153)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=156153)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=156153)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=156153)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_dc644262_lightning\n",
      "\u001b[36m(_trainable pid=156153)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=156153)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=156153)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=156153)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=156768)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=156768)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=156768)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=156768)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=156768)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=156768)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=156768)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=156768)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=156768)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a77fc8aa_lightning\n",
      "\u001b[36m(_trainable pid=156768)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=156768)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=156768)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=156768)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=157307)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=157307)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=157307)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=157307)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=157307)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=157307)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=157307)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=157307)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=157307)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f6a2fe3e_lightning\n",
      "\u001b[36m(_trainable pid=157307)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=157307)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=157307)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=157307)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=157868)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=157868)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=157868)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=157868)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=157868)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=157868)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=157868)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=157868)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=157868)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_dfbe79cc_lightning\n",
      "\u001b[36m(_trainable pid=157868)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=157868)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=157868)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=157868)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=158415)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=158415)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=158415)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=158415)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=158415)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=158415)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=158415)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=158415)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=158415)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f9e1978d_lightning\n",
      "\u001b[36m(_trainable pid=158415)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=158415)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=158415)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=158415)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=158990)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=158990)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=158990)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=158990)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=158990)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=158990)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=158990)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=158990)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=158990)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6b6b046e_lightning\n",
      "\u001b[36m(_trainable pid=158990)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=158990)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=158990)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=158990)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=159536)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=159536)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=159536)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=159536)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=159536)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=159536)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=159536)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=159536)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=159536)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_82decbdd_lightning\n",
      "\u001b[36m(_trainable pid=159536)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=159536)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=159536)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=159536)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=160012)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=160012)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=160012)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=160012)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=160012)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=160012)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=160012)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=160012)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=160012)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a13acb18_lightning\n",
      "\u001b[36m(_trainable pid=160012)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=160012)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=160012)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=160012)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=160564)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=160564)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=160564)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=160564)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=160564)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=160564)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=160564)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=160564)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=160564)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_40daa225_lightning\n",
      "\u001b[36m(_trainable pid=160564)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=160564)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=160564)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=160564)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=161163)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=161163)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=161163)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=161163)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=161163)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=161163)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=161163)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=161163)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=161163)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_8636815d_lightning\n",
      "\u001b[36m(_trainable pid=161163)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=161163)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=161163)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=161163)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=161796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=161796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=161796)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=161796)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=161796)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=161796)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=161796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=161796)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=161796)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_32333f23_lightning\n",
      "\u001b[36m(_trainable pid=161796)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=161796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=161796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=161796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=162406)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=162406)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=162406)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=162406)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=162406)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=162406)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=162406)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=162406)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=162406)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_59f7ffd1_lightning\n",
      "\u001b[36m(_trainable pid=162406)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=162406)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=162406)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=162406)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=163091)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=163091)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=163091)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=163091)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=163091)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=163091)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=163091)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=163091)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=163091)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c2201c62_lightning\n",
      "\u001b[36m(_trainable pid=163091)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=163091)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=163091)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=163091)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=163638)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=163638)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=163638)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=163638)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=163638)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=163638)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=163638)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=163638)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=163638)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3eace893_lightning\n",
      "\u001b[36m(_trainable pid=163638)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=163638)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=163638)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=163638)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=164137)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=164137)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=164137)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=164137)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=164137)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=164137)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=164137)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=164137)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=164137)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_74253464_lightning\n",
      "\u001b[36m(_trainable pid=164137)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=164137)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=164137)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=164137)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=164657)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=164657)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=164657)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=164657)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=164657)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=164657)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=164657)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=164657)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=164657)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c9db84a7_lightning\n",
      "\u001b[36m(_trainable pid=164657)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=164657)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=164657)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=164657)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_98e7defd_lightning\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165222)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=165725)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=165725)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=165725)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=165725)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=165725)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=165725)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=165725)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=165725)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=165725)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_dd8df845_lightning\n",
      "\u001b[36m(_trainable pid=165725)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=165725)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=165725)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=165725)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_aa1c7a53_lightning\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166329)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=166981)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=166981)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=166981)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=166981)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=166981)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=166981)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=166981)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=166981)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=166981)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c9802942_lightning\n",
      "\u001b[36m(_trainable pid=166981)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=166981)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=166981)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=166981)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=167494)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=167494)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=167494)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=167494)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=167494)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=167494)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=167494)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=167494)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=167494)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b8919032_lightning\n",
      "\u001b[36m(_trainable pid=167494)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=167494)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=167494)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=167494)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=168087)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=168087)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=168087)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=168087)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=168087)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=168087)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=168087)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=168087)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=168087)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5c8b30fe_lightning\n",
      "\u001b[36m(_trainable pid=168087)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=168087)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=168087)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=168087)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=168633)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=168633)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=168633)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=168633)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=168633)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=168633)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=168633)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=168633)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=168633)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cfe73709_lightning\n",
      "\u001b[36m(_trainable pid=168633)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=168633)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=168633)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=168633)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=169109)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=169109)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=169109)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=169109)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=169109)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=169109)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=169109)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=169109)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=169109)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_810fa116_lightning\n",
      "\u001b[36m(_trainable pid=169109)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=169109)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=169109)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=169109)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=169685)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=169685)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=169685)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=169685)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=169685)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=169685)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=169685)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=169685)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=169685)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4562155c_lightning\n",
      "\u001b[36m(_trainable pid=169685)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=169685)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=169685)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=169685)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=170376)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=170376)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=170376)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=170376)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=170376)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=170376)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=170376)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=170376)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=170376)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_0af495c1_lightning\n",
      "\u001b[36m(_trainable pid=170376)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=170376)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=170376)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=170376)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_126c602c_lightning\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=171457)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=172436)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=172436)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=172436)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=172436)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=172436)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=172436)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=172436)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=172436)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=172436)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_270ac612_lightning\n",
      "\u001b[36m(_trainable pid=172436)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=172436)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=172436)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=172436)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=173544)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=173544)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=173544)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=173544)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=173544)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=173544)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=173544)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=173544)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=173544)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_59545373_lightning\n",
      "\u001b[36m(_trainable pid=173544)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=173544)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=173544)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=173544)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a67b7663_lightning\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=174531)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=175666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=175666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=175666)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=175666)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=175666)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=175666)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=175666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=175666)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=175666)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_ea8d285f_lightning\n",
      "\u001b[36m(_trainable pid=175666)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=175666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=175666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=175666)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=176608)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=176608)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=176608)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=176608)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=176608)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=176608)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=176608)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=176608)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=176608)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_881603c2_lightning\n",
      "\u001b[36m(_trainable pid=176608)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=176608)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=176608)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=176608)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_33a519ac_lightning\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=177671)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=178242)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=178242)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=178242)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=178242)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=178242)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=178242)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=178242)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=178242)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=178242)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_2f33f74f_lightning\n",
      "\u001b[36m(_trainable pid=178242)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=178242)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=178242)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=178242)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=178941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=178941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=178941)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=178941)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=178941)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=178941)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=178941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=178941)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=178941)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_461e0483_lightning\n",
      "\u001b[36m(_trainable pid=178941)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=178941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=178941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=178941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=180472)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=180472)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=180472)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=180472)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=180472)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=180472)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=180472)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=180472)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=180472)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_bc5c39b5_lightning\n",
      "\u001b[36m(_trainable pid=180472)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=180472)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=180472)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=180472)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=181397)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=181397)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=181397)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=181397)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=181397)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=181397)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=181397)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=181397)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=181397)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_64533105_lightning\n",
      "\u001b[36m(_trainable pid=181397)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=181397)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=181397)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=181397)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=182880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=182880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=182880)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=182880)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=182880)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=182880)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=182880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=182880)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=182880)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_eeda4121_lightning\n",
      "\u001b[36m(_trainable pid=182880)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=182880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=182880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=182880)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d7461d86_lightning\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=183910)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=185222)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=185222)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=185222)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=185222)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=185222)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=185222)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=185222)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=185222)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=185222)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_82257724_lightning\n",
      "\u001b[36m(_trainable pid=185222)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=185222)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=185222)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=185222)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=186820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=186820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=186820)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=186820)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=186820)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=186820)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=186820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=186820)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=186820)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_ecdfba81_lightning\n",
      "\u001b[36m(_trainable pid=186820)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=186820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=186820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=186820)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b7702d6e_lightning\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=187634)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=188822)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=188822)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=188822)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=188822)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=188822)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=188822)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=188822)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=188822)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=188822)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_355f0c79_lightning\n",
      "\u001b[36m(_trainable pid=188822)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=188822)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=188822)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=188822)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5e1d5467_lightning\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=189899)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=190980)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=190980)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=190980)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=190980)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=190980)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=190980)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=190980)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=190980)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=190980)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_64af5fe1_lightning\n",
      "\u001b[36m(_trainable pid=190980)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=190980)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=190980)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=190980)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=191878)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=191878)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=191878)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=191878)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=191878)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=191878)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=191878)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=191878)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=191878)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_0398a046_lightning\n",
      "\u001b[36m(_trainable pid=191878)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=191878)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=191878)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=191878)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=192899)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=192899)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=192899)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=192899)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=192899)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=192899)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=192899)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=192899)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=192899)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_fb8e5789_lightning\n",
      "\u001b[36m(_trainable pid=192899)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=192899)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=192899)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=192899)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=193774)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=193774)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=193774)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=193774)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=193774)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=193774)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=193774)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=193774)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=193774)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e9a60cce_lightning\n",
      "\u001b[36m(_trainable pid=193774)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=193774)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=193774)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=193774)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=194562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=194562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=194562)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=194562)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=194562)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=194562)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=194562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=194562)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=194562)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_12fb9bb8_lightning\n",
      "\u001b[36m(_trainable pid=194562)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=194562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=194562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=194562)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=195207)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=195207)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=195207)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=195207)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=195207)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=195207)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=195207)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=195207)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=195207)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_67fd8061_lightning\n",
      "\u001b[36m(_trainable pid=195207)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=195207)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=195207)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=195207)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=195946)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=195946)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=195946)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=195946)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=195946)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=195946)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=195946)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=195946)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=195946)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b2d6cc38_lightning\n",
      "\u001b[36m(_trainable pid=195946)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=195946)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=195946)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=195946)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5af50a8e_lightning\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=196740)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=197474)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=197474)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=197474)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=197474)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=197474)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=197474)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=197474)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=197474)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=197474)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e587e89e_lightning\n",
      "\u001b[36m(_trainable pid=197474)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=197474)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=197474)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=197474)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=198188)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=198188)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=198188)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=198188)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=198188)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=198188)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=198188)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=198188)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=198188)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9738ee07_lightning\n",
      "\u001b[36m(_trainable pid=198188)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=198188)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=198188)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=198188)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=198923)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=198923)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=198923)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=198923)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=198923)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=198923)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=198923)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=198923)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=198923)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_616b4959_lightning\n",
      "\u001b[36m(_trainable pid=198923)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=198923)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=198923)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=198923)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_644ce835_lightning\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=199737)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=200866)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=200866)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=200866)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=200866)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=200866)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=200866)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=200866)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=200866)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=200866)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5a1415fc_lightning\n",
      "\u001b[36m(_trainable pid=200866)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=200866)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=200866)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=200866)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=201519)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=201519)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=201519)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=201519)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=201519)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=201519)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=201519)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=201519)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=201519)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_bf02a98e_lightning\n",
      "\u001b[36m(_trainable pid=201519)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=201519)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=201519)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=201519)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=202521)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=202521)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=202521)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=202521)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=202521)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=202521)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=202521)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=202521)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=202521)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1505c273_lightning\n",
      "\u001b[36m(_trainable pid=202521)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=202521)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=202521)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=202521)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=203487)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=203487)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=203487)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=203487)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=203487)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=203487)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=203487)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=203487)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=203487)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cc8912b7_lightning\n",
      "\u001b[36m(_trainable pid=203487)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=203487)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=203487)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=203487)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=204373)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=204373)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=204373)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=204373)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=204373)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=204373)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=204373)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=204373)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=204373)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f8f24b94_lightning\n",
      "\u001b[36m(_trainable pid=204373)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=204373)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=204373)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=204373)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=205096)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=205096)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=205096)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=205096)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=205096)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=205096)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=205096)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=205096)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=205096)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_90b4f5d5_lightning\n",
      "\u001b[36m(_trainable pid=205096)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=205096)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=205096)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=205096)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=205941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=205941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=205941)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=205941)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=205941)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=205941)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=205941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=205941)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=205941)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d873fe1e_lightning\n",
      "\u001b[36m(_trainable pid=205941)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=205941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=205941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=205941)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=206900)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=206900)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=206900)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=206900)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=206900)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=206900)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=206900)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=206900)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=206900)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_93d8bf1d_lightning\n",
      "\u001b[36m(_trainable pid=206900)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=206900)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=206900)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=206900)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=207694)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=207694)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=207694)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=207694)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=207694)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=207694)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=207694)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=207694)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=207694)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_7631affc_lightning\n",
      "\u001b[36m(_trainable pid=207694)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=207694)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=207694)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=207694)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_36d7d640_lightning\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=208551)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=209499)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=209499)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=209499)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=209499)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=209499)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=209499)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=209499)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=209499)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=209499)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e2425007_lightning\n",
      "\u001b[36m(_trainable pid=209499)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=209499)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=209499)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=209499)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=210573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=210573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=210573)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=210573)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=210573)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=210573)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=210573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=210573)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=210573)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9f804d47_lightning\n",
      "\u001b[36m(_trainable pid=210573)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=210573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=210573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=210573)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=211508)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=211508)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=211508)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=211508)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=211508)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=211508)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=211508)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=211508)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=211508)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_41caab0c_lightning\n",
      "\u001b[36m(_trainable pid=211508)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=211508)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=211508)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=211508)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=212208)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=212208)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=212208)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=212208)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=212208)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=212208)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=212208)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=212208)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=212208)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_72cb2b08_lightning\n",
      "\u001b[36m(_trainable pid=212208)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=212208)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=212208)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=212208)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=213023)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=213023)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=213023)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=213023)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=213023)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=213023)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=213023)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=213023)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=213023)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_df11174e_lightning\n",
      "\u001b[36m(_trainable pid=213023)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=213023)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=213023)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=213023)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=213523)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=213523)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=213523)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=213523)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=213523)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=213523)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=213523)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=213523)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=213523)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c00e1608_lightning\n",
      "\u001b[36m(_trainable pid=213523)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=213523)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=213523)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=213523)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=214126)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=214126)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=214126)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=214126)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=214126)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=214126)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=214126)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=214126)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=214126)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_53760970_lightning\n",
      "\u001b[36m(_trainable pid=214126)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=214126)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=214126)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=214126)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=214774)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=214774)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=214774)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=214774)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=214774)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=214774)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=214774)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=214774)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=214774)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_964a4994_lightning\n",
      "\u001b[36m(_trainable pid=214774)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=214774)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=214774)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=214774)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c16b8e62_lightning\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=215489)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=216277)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=216277)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=216277)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=216277)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=216277)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=216277)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=216277)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=216277)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=216277)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_da4aff5d_lightning\n",
      "\u001b[36m(_trainable pid=216277)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=216277)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=216277)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=216277)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=216949)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=216949)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=216949)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=216949)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=216949)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=216949)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=216949)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=216949)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=216949)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_af182809_lightning\n",
      "\u001b[36m(_trainable pid=216949)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=216949)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=216949)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=216949)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f66a7235_lightning\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=217735)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=218485)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=218485)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=218485)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=218485)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=218485)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=218485)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=218485)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=218485)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=218485)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_51ca5757_lightning\n",
      "\u001b[36m(_trainable pid=218485)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=218485)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=218485)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=218485)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=219253)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=219253)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=219253)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=219253)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=219253)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=219253)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=219253)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=219253)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=219253)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_fd26dee4_lightning\n",
      "\u001b[36m(_trainable pid=219253)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=219253)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=219253)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=219253)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=219956)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=219956)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=219956)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=219956)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=219956)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=219956)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=219956)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=219956)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=219956)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_1505e318_lightning\n",
      "\u001b[36m(_trainable pid=219956)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=219956)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=219956)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=219956)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=220564)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=220564)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=220564)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=220564)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=220564)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=220564)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=220564)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=220564)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=220564)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_e305f12b_lightning\n",
      "\u001b[36m(_trainable pid=220564)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=220564)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=220564)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=220564)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=221181)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=221181)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=221181)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=221181)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=221181)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=221181)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=221181)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=221181)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=221181)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_fbf878e3_lightning\n",
      "\u001b[36m(_trainable pid=221181)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=221181)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=221181)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=221181)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=221689)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=221689)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=221689)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=221689)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=221689)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=221689)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=221689)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=221689)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=221689)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c20da3c9_lightning\n",
      "\u001b[36m(_trainable pid=221689)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=221689)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=221689)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=221689)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=222495)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=222495)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=222495)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=222495)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=222495)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=222495)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=222495)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=222495)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=222495)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_120395fb_lightning\n",
      "\u001b[36m(_trainable pid=222495)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=222495)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=222495)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=222495)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=223085)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=223085)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=223085)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=223085)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=223085)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=223085)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=223085)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=223085)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=223085)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_40e99c0d_lightning\n",
      "\u001b[36m(_trainable pid=223085)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=223085)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=223085)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=223085)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=223663)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=223663)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=223663)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=223663)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=223663)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=223663)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=223663)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=223663)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=223663)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b7f7597d_lightning\n",
      "\u001b[36m(_trainable pid=223663)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=223663)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=223663)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=223663)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=224187)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=224187)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=224187)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=224187)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=224187)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=224187)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=224187)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=224187)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=224187)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_db3d2f98_lightning\n",
      "\u001b[36m(_trainable pid=224187)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=224187)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=224187)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=224187)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_6b54978a_lightning\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=224789)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=225395)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=225395)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=225395)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=225395)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=225395)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=225395)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=225395)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=225395)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=225395)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9b0c04e0_lightning\n",
      "\u001b[36m(_trainable pid=225395)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=225395)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=225395)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=225395)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=225920)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=225920)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=225920)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=225920)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=225920)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=225920)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=225920)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=225920)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=225920)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_bd5ede2c_lightning\n",
      "\u001b[36m(_trainable pid=225920)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=225920)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=225920)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=225920)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "2024-08-06 16:08:02,782\tERROR tune_controller.py:1331 -- Trial task failed for trial _trainable_bd5ede2c\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/worker.py\", line 861, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=225920, ip=172.26.93.179, actor_id=0d1011df5313a1fc1a5e28c601000000, repr=_trainable)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 98, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/autotune/_manager.py\", line 439, in _trainable\n",
      "    model.train(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/model/base/_training_mixin.py\", line 143, in train\n",
      "    return runner()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainrunner.py\", line 98, in __call__\n",
      "    self.trainer.fit(self.training_plan, self.data_splitter)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainer.py\", line 219, in fit\n",
      "    super().fit(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 989, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 1035, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n",
      "    self.advance()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 137, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 285, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/utilities.py\", line 182, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 134, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 391, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 309, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 403, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainingplans.py\", line 360, in validation_step\n",
      "    _, _, scvi_loss = self.forward(batch, loss_kwargs=self.loss_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainingplans.py\", line 278, in forward\n",
      "    return self.module(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 41, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 203, in forward\n",
      "    return _generic_forward(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 739, in _generic_forward\n",
      "    inference_outputs = module.inference(**inference_inputs, **inference_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 41, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 307, in inference\n",
      "    return self._regular_inference(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 41, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/_vae.py\", line 323, in _regular_inference\n",
      "    qz, z = self.z_encoder(encoder_input, batch_index, *categorical_input)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/nn/_base_components.py\", line 282, in forward\n",
      "    dist = Normal(q_m, q_v.sqrt())\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/distributions/normal.py\", line 57, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 70, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter loc (Tensor of shape (55, 3)) of distribution Normal(loc: torch.Size([55, 3]), scale: torch.Size([55, 3])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0')\n",
      "\u001b[36m(_trainable pid=226659)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=226659)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=226659)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=226659)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=226659)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=226659)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=226659)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=226659)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=226659)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_46091964_lightning\n",
      "\u001b[36m(_trainable pid=226659)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=226659)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=226659)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=226659)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5b397612_lightning\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=227159)\u001b[0m NaN or Inf found in input tensor.\n",
      "2024-08-06 16:08:22,964\tERROR tune_controller.py:1331 -- Trial task failed for trial _trainable_5b397612\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/_private/worker.py\", line 861, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=227159, ip=172.26.93.179, actor_id=82fcd33635704ca9e6997e5401000000, repr=_trainable)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 98, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/trainable/util.py\", line 130, in inner\n",
      "    return trainable(config, **fn_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/autotune/_manager.py\", line 439, in _trainable\n",
      "    model.train(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/model/base/_training_mixin.py\", line 143, in train\n",
      "    return runner()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainrunner.py\", line 98, in __call__\n",
      "    self.trainer.fit(self.training_plan, self.data_splitter)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainer.py\", line 219, in fit\n",
      "    super().fit(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 989, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py\", line 1035, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n",
      "    self.advance()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/core/module.py\", line 1291, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 230, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 117, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 484, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 89, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/optim/adam.py\", line 205, in step\n",
      "    loss = closure()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 104, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 140, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 126, in closure\n",
      "    step_output = self._step_fn()\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 315, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py\", line 309, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py\", line 382, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainingplans.py\", line 344, in training_step\n",
      "    _, _, scvi_loss = self.forward(batch, loss_kwargs=self.loss_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/train/_trainingplans.py\", line 278, in forward\n",
      "    return self.module(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 32, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 203, in forward\n",
      "    return _generic_forward(\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 739, in _generic_forward\n",
      "    inference_outputs = module.inference(**inference_inputs, **inference_kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 32, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_base_module.py\", line 307, in inference\n",
      "    return self._regular_inference(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/base/_decorators.py\", line 32, in auto_transfer_args\n",
      "    return fn(self, *args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/module/_vae.py\", line 323, in _regular_inference\n",
      "    qz, z = self.z_encoder(encoder_input, batch_index, *categorical_input)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/scvi/nn/_base_components.py\", line 282, in forward\n",
      "    dist = Normal(q_m, q_v.sqrt())\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/distributions/normal.py\", line 57, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/home/chzhan1/.local/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 70, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter loc (Tensor of shape (115, 3)) of distribution Normal(loc: torch.Size([115, 3]), scale: torch.Size([115, 3])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "\u001b[36m(_trainable pid=227781)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=227781)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=227781)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=227781)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=227781)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=227781)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=227781)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=227781)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=227781)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_8d564fd4_lightning\n",
      "\u001b[36m(_trainable pid=227781)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=227781)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=227781)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=227781)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=228619)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=228619)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=228619)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=228619)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=228619)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=228619)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=228619)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=228619)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=228619)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_9ee9eabc_lightning\n",
      "\u001b[36m(_trainable pid=228619)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=228619)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=228619)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=228619)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=229542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=229542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=229542)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=229542)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=229542)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=229542)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=229542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=229542)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=229542)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f51c89e0_lightning\n",
      "\u001b[36m(_trainable pid=229542)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=229542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=229542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=229542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=230515)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=230515)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=230515)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=230515)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=230515)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=230515)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=230515)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=230515)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=230515)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_57dcb362_lightning\n",
      "\u001b[36m(_trainable pid=230515)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=230515)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=230515)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=230515)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=231993)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=231993)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=231993)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=231993)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=231993)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=231993)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=231993)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=231993)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=231993)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_43ab27db_lightning\n",
      "\u001b[36m(_trainable pid=231993)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=231993)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=231993)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=231993)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=232953)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=232953)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=232953)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=232953)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=232953)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=232953)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=232953)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=232953)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=232953)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_514bf8e0_lightning\n",
      "\u001b[36m(_trainable pid=232953)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=232953)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=232953)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=232953)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=233853)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=233853)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=233853)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=233853)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=233853)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=233853)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=233853)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=233853)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=233853)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_f3d7efb0_lightning\n",
      "\u001b[36m(_trainable pid=233853)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=233853)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=233853)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=233853)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=234874)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=234874)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=234874)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=234874)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=234874)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=234874)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=234874)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=234874)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=234874)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_b9ec2015_lightning\n",
      "\u001b[36m(_trainable pid=234874)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=234874)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=234874)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=234874)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=236437)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=236437)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=236437)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=236437)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=236437)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=236437)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=236437)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=236437)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=236437)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_0d75c799_lightning\n",
      "\u001b[36m(_trainable pid=236437)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=236437)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=236437)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=236437)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4470b037_lightning\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=237574)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_0516b185_lightning\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=238377)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=238985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=238985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=238985)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=238985)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=238985)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=238985)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=238985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=238985)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=238985)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_de6b1a6d_lightning\n",
      "\u001b[36m(_trainable pid=238985)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=238985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=238985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=238985)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_3ef37361_lightning\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=239939)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=240664)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=240664)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=240664)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=240664)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=240664)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=240664)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=240664)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=240664)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=240664)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_597bad05_lightning\n",
      "\u001b[36m(_trainable pid=240664)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=240664)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=240664)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=240664)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=241586)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=241586)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=241586)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=241586)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=241586)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=241586)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=241586)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=241586)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=241586)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d66b562b_lightning\n",
      "\u001b[36m(_trainable pid=241586)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=241586)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=241586)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=241586)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=242297)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=242297)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=242297)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=242297)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=242297)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=242297)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=242297)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=242297)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=242297)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_7785437f_lightning\n",
      "\u001b[36m(_trainable pid=242297)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=242297)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=242297)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=242297)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=243008)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=243008)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=243008)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=243008)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=243008)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=243008)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=243008)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=243008)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=243008)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5bcf1f22_lightning\n",
      "\u001b[36m(_trainable pid=243008)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=243008)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=243008)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=243008)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=243681)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=243681)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=243681)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=243681)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=243681)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=243681)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=243681)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=243681)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=243681)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a70a5c07_lightning\n",
      "\u001b[36m(_trainable pid=243681)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=243681)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=243681)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=243681)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=244212)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=244212)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=244212)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=244212)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=244212)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=244212)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=244212)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=244212)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=244212)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_5c3ef904_lightning\n",
      "\u001b[36m(_trainable pid=244212)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=244212)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=244212)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=244212)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=244783)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=244783)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=244783)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=244783)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=244783)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=244783)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=244783)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=244783)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=244783)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d76cec64_lightning\n",
      "\u001b[36m(_trainable pid=244783)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=244783)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=244783)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=244783)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=245311)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=245311)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=245311)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=245311)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=245311)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=245311)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=245311)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=245311)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=245311)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_384ebd5a_lightning\n",
      "\u001b[36m(_trainable pid=245311)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=245311)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=245311)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=245311)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_66b04a9f_lightning\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=245907)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=246470)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=246470)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=246470)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=246470)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=246470)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=246470)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=246470)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=246470)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=246470)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_de549216_lightning\n",
      "\u001b[36m(_trainable pid=246470)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=246470)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=246470)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=246470)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=247040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=247040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=247040)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=247040)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=247040)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=247040)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=247040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=247040)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=247040)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_4fbc7325_lightning\n",
      "\u001b[36m(_trainable pid=247040)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=247040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=247040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=247040)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=247636)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=247636)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=247636)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=247636)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=247636)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=247636)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=247636)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=247636)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=247636)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_2ba18a8c_lightning\n",
      "\u001b[36m(_trainable pid=247636)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=247636)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=247636)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=247636)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=248175)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=248175)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=248175)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=248175)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=248175)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=248175)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=248175)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=248175)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=248175)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a746063b_lightning\n",
      "\u001b[36m(_trainable pid=248175)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=248175)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=248175)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=248175)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=248742)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=248742)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=248742)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=248742)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=248742)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=248742)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=248742)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=248742)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=248742)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_bda20f33_lightning\n",
      "\u001b[36m(_trainable pid=248742)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=248742)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=248742)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=248742)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=249331)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=249331)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=249331)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=249331)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=249331)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=249331)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=249331)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=249331)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=249331)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_31bdfd3b_lightning\n",
      "\u001b[36m(_trainable pid=249331)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=249331)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=249331)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=249331)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=249973)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=249973)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=249973)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=249973)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=249973)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=249973)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=249973)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=249973)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=249973)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_06b2dd31_lightning\n",
      "\u001b[36m(_trainable pid=249973)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=249973)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=249973)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=249973)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=250542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=250542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=250542)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=250542)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=250542)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=250542)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=250542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=250542)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=250542)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_8b09f1a1_lightning\n",
      "\u001b[36m(_trainable pid=250542)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=250542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=250542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=250542)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=251043)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=251043)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=251043)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=251043)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=251043)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=251043)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=251043)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=251043)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=251043)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_cdae1d36_lightning\n",
      "\u001b[36m(_trainable pid=251043)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=251043)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=251043)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=251043)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=251634)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=251634)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=251634)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=251634)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=251634)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=251634)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=251634)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=251634)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=251634)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_ff61c761_lightning\n",
      "\u001b[36m(_trainable pid=251634)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=251634)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=251634)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=251634)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=252234)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=252234)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=252234)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=252234)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=252234)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=252234)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=252234)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=252234)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=252234)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_127050a4_lightning\n",
      "\u001b[36m(_trainable pid=252234)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=252234)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=252234)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=252234)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=252798)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=252798)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=252798)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=252798)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=252798)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=252798)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=252798)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=252798)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=252798)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_80c823e3_lightning\n",
      "\u001b[36m(_trainable pid=252798)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=252798)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=252798)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=252798)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_7e7c6ded_lightning\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=253424)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=254070)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=254070)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=254070)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=254070)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=254070)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=254070)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=254070)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=254070)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=254070)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_54c288a9_lightning\n",
      "\u001b[36m(_trainable pid=254070)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=254070)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=254070)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=254070)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=254572)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=254572)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=254572)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=254572)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=254572)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=254572)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=254572)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=254572)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=254572)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_d24b2023_lightning\n",
      "\u001b[36m(_trainable pid=254572)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=254572)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=254572)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=254572)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_591f9cff_lightning\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=255305)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=256295)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=256295)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=256295)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=256295)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=256295)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=256295)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=256295)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=256295)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=256295)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c860072f_lightning\n",
      "\u001b[36m(_trainable pid=256295)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=256295)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=256295)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=256295)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=257082)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=257082)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=257082)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=257082)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=257082)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=257082)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=257082)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=257082)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=257082)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_dd67f3eb_lightning\n",
      "\u001b[36m(_trainable pid=257082)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=257082)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=257082)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=257082)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=258024)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=258024)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=258024)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=258024)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=258024)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=258024)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=258024)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=258024)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=258024)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_70f17149_lightning\n",
      "\u001b[36m(_trainable pid=258024)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=258024)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=258024)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=258024)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=259316)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=259316)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=259316)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=259316)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=259316)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=259316)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=259316)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=259316)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=259316)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_923b0c5b_lightning\n",
      "\u001b[36m(_trainable pid=259316)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=259316)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=259316)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=259316)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=260148)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=260148)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=260148)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=260148)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=260148)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=260148)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=260148)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=260148)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=260148)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_455f0265_lightning\n",
      "\u001b[36m(_trainable pid=260148)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=260148)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=260148)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=260148)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=261202)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=261202)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=261202)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=261202)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=261202)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=261202)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=261202)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=261202)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=261202)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_0deed226_lightning\n",
      "\u001b[36m(_trainable pid=261202)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=261202)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=261202)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=261202)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a1570f0f_lightning\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=261973)\u001b[0m NaN or Inf found in input tensor.\n",
      "\u001b[36m(_trainable pid=262765)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=262765)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=262765)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=262765)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=262765)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=262765)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=262765)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=262765)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=262765)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a7e9e153_lightning\n",
      "\u001b[36m(_trainable pid=262765)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=262765)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=262765)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=262765)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=263680)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=263680)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=263680)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=263680)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=263680)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=263680)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=263680)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=263680)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=263680)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_c9e3a260_lightning\n",
      "\u001b[36m(_trainable pid=263680)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=263680)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=263680)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=263680)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=264471)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=264471)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=264471)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=264471)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=264471)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=264471)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=264471)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=264471)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=264471)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_a66e983a_lightning\n",
      "\u001b[36m(_trainable pid=264471)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=264471)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=264471)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=264471)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=265759)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=265759)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=265759)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=265759)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=265759)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=265759)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=265759)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=265759)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=265759)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_68b6cee5_lightning\n",
      "\u001b[36m(_trainable pid=265759)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=265759)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=265759)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=265759)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=266796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/ray/tune/integration/pytorch_lightning.py:198: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
      "\u001b[36m(_trainable pid=266796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=266796)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(_trainable pid=266796)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(_trainable pid=266796)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(_trainable pid=266796)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(_trainable pid=266796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/chzhan1/.local/lib/python3.9/site-packages/ray ...\n",
      "\u001b[36m(_trainable pid=266796)\u001b[0m You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(_trainable pid=266796)\u001b[0m Missing logger folder: /home/chzhan1/Python/scvi_log/autotune/2024-08-06_14-40-35_scvi/_trainable_fc11a442_lightning\n",
      "\u001b[36m(_trainable pid=266796)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(_trainable pid=266796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(_trainable pid=266796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(_trainable pid=266796)\u001b[0m /home/chzhan1/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "results = scvi_tuner.fit(\n",
    "    adata,\n",
    "    metric=\"validation_loss\",\n",
    "    num_samples = 1000,\n",
    "    max_epochs = 300,\n",
    "    resources = {\"cpu\": 64, \"gpu\": 1},\n",
    "    search_space = {\n",
    "    \"n_hidden\": tune.choice([64, 128, 256, 512, 1024, 2048]),\n",
    "    \"n_layers\": tune.choice([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n",
    "    \"n_latent\": 3,\n",
    "    \"dropout_rate\" : tune.choice([0.1, 0.15, 0.2, 0.25, 0.3]),\n",
    "    \"dispersion\": \"gene-label\",\n",
    "    \"gene_likelihood\": 'nb',\n",
    "    # \"latent_distribution\" : tune.choice([\"normal\"]),\n",
    "    # \"dispersion\":tune.choice([\"gene\", \"gene-cell\", \"gene-batch\"]),\n",
    "    # \"gene_likelihood\": tune.choice([\"nb\", \"zinb\"]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc943aa7-e961-4087-a8ac-a9a2faa82828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_hidden': 64, 'n_layers': 5, 'n_latent': 3, 'dropout_rate': 0.3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d546ec98-310f-4d77-8b87-f8204c8feefb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'plan_kwargs': {'lr': 0.009365569478410328}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.train_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff2673-6f22-4ac8-8da3-98905fbda663",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_hidden': 64, 'n_layers': 5, 'n_latent': 3, 'dropout_rate': 0.3}\n",
      "{'plan_kwargs': {'lr': 0.009365569478410328}}\n"
     ]
    }
   ],
   "source": [
    "print(results.model_kwargs)\n",
    "print(results.train_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
